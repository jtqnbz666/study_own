本周：合并stored和文库入hbase流程， 产品月度，会员测试数据，星盘数据，自媒体问答站内数据入hbase



流程合并测试执行；python3 kv_to_hbase.py --action=vertical_data --table=wenku --data_from=inc --conf=kv_to_hbase.ini



金锄头： www.jinchutou.com
新浪爱问：http://ishare.iask.sina.com.cn
二一教育：www.21cnjy.com
查字典：wendang.chazidian.com
糖果：www.tgppt.com
61： www.16pic.com
齐齐文库： www.qiqiwenku.com
小Q办公哈：www.xqppt.com

**取数据：**
如果太大
hcat hdfs://namenodefd4v.qss.zzzc.qihoo.net:9000/home/eng/jiangtao5/tmp/16pic/part-00000 | head -200000 > testdata 

文库需要url资源，需要拼接，https://wenku.so.com/d/ + md5(url)



文库全量据
/home/maintable/vertical/library_document_merge_all/20230427

idc测试环境

idclog测试环境 slog14.safe.zzzc.qihoo.net把编译的动态库拷贝到 /home/s/apps/CloudSearch/kaka/topology/idclog_kv_primary_test_zzzc2执行1、/home/s/apps/CloudSearch/kaka-worker-idclog/bin/serverctl start2、sh /home/s/apps/CloudSearch/kaka-worker-idclog/run_online.sh



新开机密码: Nswdsm58@wh010921

登录服务器密码: Nswdsm58@wh

极库云git代码，https 密码： Nswdsm58_

计算平台创建文件title_vector_put1/title_vector_put/vector2hbase/vertical_data_transfer/testwenku_fields.ini

dev103.se.corp.qihoo.net

dev106.se.corp.qihoo.net

offline01.qsst.zzzc.qihoo.net			sudo -u eng -H bash

docker14v.search.corp.qihoo.net   //go

slog25.safe.zzzc.qihoo.net  //idc

slog14.safe.zzzc.qihoo.net  //idc 测试

slog23.safe.zzzc.qihoo.net

sudo /usr/local/go1.16/go/bin/go env -w GOPATH="/home/jiangtao5/"





21教育，齐齐文库，差字典
(site == "https://wendang.chazidian.com/" or site == "www.21cnjy.com" or site == "www.qiqiwenku.com")): 爱问的数据是以byte为单位的，其他是KB为单位



验证数据是否成功入hbase

hbase2 shell   	get 'library_document_down_file','md5'

求md5 和 sha1

~~~ |
echo -n "xx" | md5sum
echo -n "xx" | sha1sum
~~~

小Q文档；./qbus_write_robin_file part-00000 shbt_priv_scp_2 wenku_xqppt_download

/home/hdp-qssweb/data/onebox/zhanzhang_data
星盘的数据最终落到hdfs上这个路径，我们要根据刷新号把数据摘出来

hdfs://namenodefd4v.qss.zzzc.qihoo.net:9000/home/eng/jiangtao5/hbase_xinpan/qiqiwenku_2-22-23_4-11-12
我rename成这样了，下次就这样命名吧，不容易弄混

**巨坑， 要把自己用户的文件放到eng下对应的自己的账户下才可以上传文件到hadoop**

问答数据源头：
/home/eng/huangjianfeng/wenda/index/20230303/merge_fresh_inc_data_and_split_bucket_step_all

**问答数据存储：**hdfs://namenodefd4v.qss.zzzc.qihoo.net:9000/home/eng/jiangtao5/wenda

智选数据源头：
/home/qss/data/qss_engine_log/zhixuan_data_index/

**智选数据存储**
hdfs://namenodefd4v.qss.zzzc.qihoo.net:9000/home/eng/jiangtao5/wenda



查看集群配置

![1678174796841](C:\Users\jiangtao5\AppData\Roaming\Typora\typora-user-images\1678174796841.png)



测试flink

先拉一份数据过来

fresh数据
/home/eng/makaiquan/wenda/fresh/ 
hadoop dfs -get /home/eng/makaiquan/wenda/fresh/20230228/part-0-2

inc数据
/home/eng/makaiquan/wenda/inc/ 
hadoop dfs -ls /home/eng/makaiquan/wenda/inc/20230215/part-0-99

 export LD_LIBRARY_PATH=.:$LD_LIBRARY_PATH

~~~
idclog生产数据
./qbus_write_robin_file idctestdata shbt_priv_scp_2 stored_diff_input_test

idclog消费数据
./kafka-console-consumer.sh --bootstrap-server  10
.217.116.31:39092 --topic stored_diff_output_test --group test

生产者 
 
 fresh 流程
  ./qbus_write_robin_file ./test.txt zzzc_priv_qssl2eng1 zzzc_priv_qssl2eng_common_test2
 
 inc流程
 ./qbus_write_robin_file ./test-inc.txt shbt_priv_scp_2 shbt_priv_scp_2_comon_test2
 
 控制台
 ./kafka-console-producer.sh --broker-list
  --topic zzzc_priv_qssl2eng_comon_test2
 
 
消费者


fresh流程

./kafka-console-consumer.sh --bootstrap-server 10.172.95.63:39092 --topic 
zzzc_priv_qssl2eng_vertical_wenda_fresh_output_test  --group wenda-jt

inc流程
./kafka-console-consumer.sh --bootstrap-server  10.217.116.31:39092 --topic shbt_priv_scp_2_wenda_inc_sink --group wenda-jt


./kafka-console-consumer.sh --bootstrap-server 10.173.92.202:2181,10.173.92.203:2181,10.173.92.218:2181  --topic 
~~~

配置文件信息：hadoop dfs -cat /home/maintable/vertical/config/wenda-test.json

![1678241841281](C:\Users\jiangtao5\AppData\Roaming\Typora\typora-user-images\1678241841281.png)

集群：zzzc_priv_qssl2eng1

ip：10.172.95.63:39092

**对于fresh流**

生产主题 ;
zzzc_priv_qssl2eng_common_test2
消费主题;
zzzc_priv_qssl2eng_vertical_wenda_fresh_output_test

**对于inc流**

生产主题
shbt_priv_scp_2_comon_test2
消费主题
shbt_priv_scp_2_wenda_inc_sink





inc 和 fresh 数据位置
/home/eng/makaiquan/wenda/fresh/   fresh每天的数据 hadoop上
/home/eng/makaiquan/wenda/inc/       inc 每天的数据



```
zzzc_priv_qssl2eng", "10.173.92.202:2181,10.173.92.203:2181,10.173.92.218:2181
```





申请代码权限

https://cas.src.corp.qihoo.net/index.php/Auth/Permission/addIndex

wiki权限找运维

flink流

[qdev - Revision 285538: /qsearch/branches/flink_stream (qihoo.net)](https://se.src.corp.qihoo.net/qdev/qsearch/branches/flink_stream/)





idc环境问题

~~~
qmodule_get -m=kaka_core -v=1.0.1
qmodule_get -m=filtercli_new -v=1.0.0
qmodule_get -m=libsearch_common -v=4.3.8
qmodule_get -m=libsearch_messages -v=3.6.7
qmodule_get -m=serializer -v=2.6.0
qmodule_get -m=CloudQueryEngine -v=1.4.10

~~~



wiki (资料)

https://wiki.so.qihoo.net/pages/viewpage.action?pageId=137907748



hulk云平台(申请服务器)

https://hulk.qihoo.net/user/host/list#
密码: Nswdsm58@wh

https://ops.qihoo.net/work-manage/system-account/order 这个更好





hadoop dfs -ls /home/maintable/vertical/config/



运行flink， hulk中点击工作台，任务开发，实时任务

奇麟大数据

http://qilin.qihoo.net/#/scheduler/projects/query?type=create



查看已经申请的代码

[qdev - Revision 285538: /qsearch/branches/flink_stream (qihoo.net)](https://se.src.corp.qihoo.net/qdev/qsearch/branches/flink_stream/)

密码：nswdsm58





安装mvn

链接：https://pan.baidu.com/s/12SuxtOXiUhNgkb0DH1eyCw
提取码：gnaf

配置bin环境

使用mvn管理jar包

pom.xml配置

```
方式一：

mvn install:install-file -Dfile=E:\workspace\vertical_wenda\lib/flink-connector-kafka-0.9_2.11-1.11.1-f2-qihoo.jar -DgroupId=org.apache.flink -DartifactId=flink-connector-kafka-0.9_2.11 -Dversion=1.0.0 -Dpackaging=jar

<groupId>flink-connector-kafka</groupId><artifactId>flink-connector-kafka</artifactId><version>1.0.0</version>


import sahded.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer09;

方式二：
把09kafka版本改为1.9.2
```



-Dfile : 源jar路径

-DgroupId : 相当于在C:\Users\jiangtao5\.m2\repository之后的路径

-DartifactId : 相当于-DgroupId之后的路径

这些值可以在flink中的pom.xml文件中看到





消费kafka数据

~~~
 ./kafka-console-consumer.sh --bootst
rap-server 10.173.194.164:39092  --topic zzzc_priv_qssl2eng_vertical_offl
ine_wenda_fresh  --group wenda-jt-test
~~~



图床项目，

从json中拿到文档的信息(docid,  url(这个url是普通用户看到的信息))，通过这些信息配合合作方提供的接口可以得到一个url(这个url就是具体的真实下载文档，用户需要付费下载的)， suda+md5(url)+sign 作为文件名字的名字，去访问我们刚才拼接出来的url得到真实文档数据，拷贝到刚才生成的文件中，用第三方sdks3将数据上传到bucket上。 此时文档就算存储好了，会把刚才生成的临时文件删除(应该是sdk中嵌入了这个步骤)

接着是下载图片，一样的使用json中拿到的信息配合合作方提供的接口，就能得到合作方的这些图片， 发送request请求，从回应中的body拿到图片数据，拷贝到本地，给这些图片命名为md5(url)_number, 然后把这些图片上传到一个具体存储数据的http服务器上，并且会返回图片对应的tuchangKey值，每个tuchangKey对应一张图片，
append(tuchuangUrl,"https://p"+strconv.Itoa(rand.Intn(10))+".qhimg.com/"+tuchangKey+".png")，使用这样的方式将图片链接放到数组中。为什么这里取了随机数，因为有多个服务器可以提供服务，随意的给这些图片配置一个就行了，最主要的是图片对应的tuchangKey值。最后把本地生成的这些图片文件删除，

最后一步，先去s3上看看第一步的文档信息下载成功没有，成功的话接着执行，刚才下载图片的时候把每一张图片的链接都保存在了一个[]string切片中，把这些关键信息设置好，一起通过一个hbase访问的接口写到hbase中，此时一条数据就处理完了