## base64

“Base64是网络上最常见的用于传输8Bit字节码的编码方式之一，Base64就是一种基于64个可打印字符来表示二进制数据的方法”。

        什么是“可打印字符”呢？为什么要用它来传输8Bit字节码呢？在回答这两个问题之前我们有必要来思考一下什么情况下需要使用到Base64？Base64一般用于在HTTP协议下传输二进制数据，由于HTTP协议是文本协议，所以在HTTP协议下传输二进制数据需要将二进制数据转换为字符数据。然而直接转换是不行的。因为网络传输只能传输可打印字符。什么是可打印字符？在ASCII码中规定，0~31、127这33个字符属于控制字符，32~126这95个字符属于可打印字符，也就是说网络传输只能传输这95个字符，不在这个范围内的字符无法传输。那么该怎么才能传输其他字符呢？其中一种方式就是使用Base64。
也就是说，如果将索引转换为对应的二进制数据的话需要至多6个Bit。然而ASCII码需要8个Bit来表示，那么怎么使用6个Bit来表示8个Bit的数据呢？6个Bit当然不能存储8个Bit的数据，但是4*6个Bit可以存储3*8个Bit的数据

![](C:\Users\ASUS\Pictures\博客图片\20180313131013494.png)





## log4cpp

关键结构， append和yaml 什么的， 有一个true或false的选择， 如果选择true把基类的方式也打印出来。 

**如何提高性能；**
减小锁粒度， 双缓冲，双队列
减少系统调用，比如fileappend那里，每次通过lseek来获取文件大小， 影响性能，可以用一个变量， 每次累加获取文件大小。

**程序崩溃如何做恢复**， coredump





## json

采用一种树的方式管理数据。

数组的形式

~~~json
简单
{
"name":"网站",
"num":3,
"sites":[ "Google", "Runoob", "Taobao" ]
}

复杂
myObj = {
    "name":"网站",
    "num":3,
    "sites": [
        { "name":"Google", "info":[ "Android", "Google 搜索", "Google 翻译" ] },
        { "name":"Runoob", "info":[ "菜鸟教程", "菜鸟工具", "菜鸟微信" ] },
        { "name":"Taobao", "info":[ "淘宝", "网购" ] }
    ]
}
~~~

 

java 处理json的细节

~~~
不管.toString(), .getString() , 还是.optString() 得到的都是没有最外层双引号的串


又一次醒悟
比如有一串数据是 {"test":123}
那么 
String str;
将这串数据放到str就是
str = "{\"test\":123}"
当我们要将这串数据转化为json格式时
JSONObject obj = new JSONOject(str)
这里需要注意本质上数据就是 {"test":123}
那么obj.toString() 也就是{"test":123}
比如现在obj.put("key", obj.toString())
那么其实上就是把{"test":123}转化为字符串的形式存储到"key"里面
所以obj就变成了
{
	"test":123
	"key":"{\"test\":123}"
}
当要去除key时
JSONObject obj2 = new JSONOject(obj.getString("key"))
仔细分析这句话，obj.getString("key") 得到的是
"{\"test\":123}"
那么自然的obj2.toString就是将"{\"test\":123}"去掉最外层引号的模样， 即{"test":123}



再次总结， 问题本质出现在
1.json将一个普通的string类型值转化为json对象，并不会有什么额外的问题
比如 String str2 = "{\"val\":123}";
 JSONObject obj2 = new JSONObject(str2);
 此时json的格式为
 {
 	"val":123
 }
 json.toString() 也是
 {
 	"val":123
 }

 2.json对象put一个字符串的时候，会为这个字符串的内容二次加上转义字符
比如上边的str2，它的原内容其实是{"val":123}, 
它会变成 "{\"val\":123}" 是以为加了外层", 
当使用obj2.put("key", str2) 时， 会主动加上一层转义字符来充当字符串本身的结果，也就是说，现在的结果变成了
{
	"val":123,
	"key": "{\"val\":123}"
}

 这也就是问题的本质， 如果使用obj2.toString()，得到的结果依然是
{
	"val":123,
	"key": "{\"val\":123}"
}
3. 将json对象取出来的时候， 会减去字符串本身的一层转义字符
比如
JSONObject obj = new JSONObject(obj2.getString("key"))
此时obj.toString()的结果是
 {
 	"val":123
 }
 我们可以发现，是在字符串本身上，减去了一层转义字符
 若将上边的
 {
	"val":123,
	"key": "{\"val\":123}"
}
变化为
{
	"val":123,
	"key": {"val":123}
}

此时 JSONObject obj3 = new JSONObject(obj2.getString("key")) 就会报错，因为"key"对应的是 {"val":123} ，并不是"{\"val\":123}"，  也就是说它并不是一个字符串，所以这种情况必须使用
 JSONObject obj3 = new JSONObject(obj2.optString("key"))
使用optString的本质意义是将{"val":123} 转化为"{\"val\":123}"， 那么这个时候同样是去掉一层转义字符
此时得到的obj3.toString()同样是
{
 	"val":123
}


我发现问题的本质了， 问题就出在json对象put一个string值的时候，会自动给你加上转义字符， 并且这个转义字符被计算成了string串本身的内容，并不会在打印出来的时候消失。
并且将json对象某一个字段的val值取出来的时候，也是真正的减去了字符串本身的那份转义字符(如果不是字符串，就必须用optString方法，这种方法会把像 {"jt":666} 转化为 "{\"jt\":666}", 然后在减去这个toString产生的转义字符，最后得到 {"jt":666}， 而如果直接就是"{\"jt\":666}"， 此时可以用getString方法，或者optString都可以, 因为已经是字符串类型了，optString不会像刚才那样又再加一层， 这里就直接去掉一层转义字符，注意这层转义字符是字符串本身的，并不是系统加的， 最后得到{"jt":666})，而不是系统自己添加的那份转义字符

普通将一个字符串转化为obj对象其实不会改变啥




以下只是帮助理解的
 
一定要记住一句话
obj.toString() 的值和json当前的值一模一样，不用考虑引号的问题

意思也就是说你使用obj.toString()出来的值， 就是json格式的串

比如JSONObject obj;
当前obj的内容为
{
 "ext":"{\"jt\":666}"
} 
你直接toString()出来， 也是这种形式
如果说当前有一个obj2
{
	"key":"123"
}
情况一：
比如执行obj2.put("val", obj.toString) 因为put一个toString方法的字符串，json会自动加上转义字符
那么此时obj2就变成了
{
	"key":"123"
	"val":"{\"ext\":\"{\\\"jt\\\":666}\"}"
}

情况二：
但如过执行的是obj2.put("val", obj)，直接put一个json对象就不会
{
	"key":"123"
	"val":{"ext":"{\"jt\":666}"}
}


当你想要取出val的时候，对于情况一直接使用
JSONObject tt = new JSONObject(obj2.getString()) 即可
此时得到的tt是
{
	{"ext":"{\"jt\":666}"}   //自动为你去除了一层转义字符
}


对于情况二
不能使用obj2.getString() 因为我刚才说了，json内容和.toString()得到的内容是一样的，不用考虑转义的问题
对于情况二
{
	"key":"123"
	"val":
}
仔细看这里的{"ext":"{\"jt\":666}"} 并没有引号包裹，所以不能使用toString()， 这里必须使用
JSONObject tt = new JSONObject(obj2.optString("val"));
optString会先把{"ext":"{\"jt\":666}"} 转化成字符串的形式
"{\"ext\":\"{\\\"jt\\\":666}\"}" ， 因为取出字符串的时候json会自动去除一层转义字符，然后就变成了{"ext":"{\"jt\":666}"} ， 这就是我们需要的格式。
情况一也可以使用optString()， 对于情况一，因为已经有了双引号包着，已经是字符串的形式了，那么取出来的时候，json同样也是自动去除一层转义字符，也就是{"ext":"{\"jt\":666}"}



场景模拟， 比如当前我接收到了一份数据 {"ext":"{\"jt\":666}"} 
String str ;
用str来承载这份数据， 因为数据里面包含引号，自然会加上转义字符，那么此时str就是
str = "{\"ext\":\"{\\\"jt\\\":666}\"}"
这里使用System.out.Println(str)得到的结果是
{"ext":"{\"jt\":666}"} 

此时我们用一个JSONObject 对象来转换它
JSONObject obj = new JSONObject(str);
已经说了很多次了， 当转化为json对象的时候，会自动去掉一层转义字符
此时obj就是{"ext":"{\"jt\":666}"} 
使用obj.toString 打印出来， 得到的结果也是{"ext":"{\"jt\":666}"} 

问题来了， 如果此时我们有一个新的对象obj2
使用obj2.put("val", obj.toString)  ，上边说过它会给toString类型自动加上一层转义字符
那么此时obj2变成了
{
 	"val":"{\"ext\":\"{\\\"jt\\\":666}\"}"
}
使用obj2.toString()打印出来同样也是
{
 	"val":"{\"ext\":\"{\\\"jt\\\":666}\"}"
}

























核心就是JSONObject对象put一个字符串进去会自动为你加上转义字符，若是直接put的json对象则不会加转义字符。
取出来的时候，会自动把转义字符给你去掉(前提是有转义字符，并且只会去掉一层)
有两种方式取出来
getString 如果键值对应的value是{"jt":666},而不是"{\"jt\":666}"，则会报错
optString 如果键值对应的value是{"jt":666}，它会自动把它转化为"{\"jt\":666}"的格式, 再取出来， 取出来会把转义字符去掉，也就变成了{"jt":666},这就是我们需要的


JSONObject obj;
obj.put("a", "{"test":"1"}");
这个put，它会自动把{"test":"1"}转变为{\"test\":\"1\"}

如果在外层又把obj.toString()作为value给put进去了，那么就会有问题，
比如现在这种情况obj.toString() 为 {"a":"{\"test\":\"1\"}"}, 如果把它作为value再次put到另外一个对象中去， 就会多加上一层转义字符，所以这时候需要对 {"a":"{\"test\":\"1\"}"} 进行处理, 把"{转化为{, 把\"转化为"。
所以如果再次put进另外一个对象时， 要么就以json本来的格式，要么就以obj.toString()的方式(需要特殊处理。)



当你要取出来的时候,
JSONObject obj2 = new JSONObject(obj.getString("a"));
obj2会自动把先前put进去的转义字符 \ 处理掉,也就是说，它会把
{\"test\":\"1\"} 转化为 {"test":"1"} 这就是正常的json格式

举例
{
"a":{"test":"1"}
}
这里用getString 和 optString的区别， 如果用getString, 但因为{"test":"1"}并没有转义字符，所以会被解析为"{"test":"1"}", 很明显这是错误的写法。 但如果用的是optString, 它会自动把{"test":"1"}转化为 
"{\"test\":\"1\"}" , 这样就是正确的格式

如果原串是
{
"a":"{\"test\":\"1\"}"
}
那么直接getString就行了， 如果使用optString也没有问题
~~~



## etcd

cetcd的key可不可以是随机数？

~~~c
可以，但不好， 比如TTL为5，如果服务在5秒内挂掉，重登，此时会有两个不同的key(因为是随机数), 对应相同的value
一般使用，ip + port;
~~~





跟mysql 对比分析
wal 相当于redolog 
snapshot相当于rdb， 当有其他服务加入集群时，通过快照的方式同步数据
boltdb 相当于存储引擎 ，如innodb等
term：leader任期时间， leader切换时term加一，全局单调递增，64bits

revision：etcd键空间版本号，key发生变更，则revison加一，全局单调递增，64bits， **用来支持MVCC**的  (每一个key对应一棵B+树，每个节点存储不同的版本,每次对这个key进行修改都会记录在这棵树上) 

~~~c
create_revision 创建数据时，对应的revision
mod_revision 数据修改时，对应的revision
version  当前的版本号，比如create_revision = 12, mod_revision = 15, 则version 就是4

linux下通过命令get key -w json | json_cpp 来查看这些信息，其中还可以看到一个raft_term字段， 表示leader的选举次数
~~~



lease grant + 秒数 创建一个租约 ，会获得一个唯一的串来标识租约
通过put key --lease="获得的串" 将key挂到这个租约上

etcd 2 和redis比较像
都是内存数据库， 同时设置key的过期时间的方式一样，这样涉及的话，如果有大量Key都是同样的过期时间，就会大量进行相同的删除key的操作，  而 etcd 3中采取了租约的方式， 多个Key挂在一个租约上，当租约到期， 所有挂在这个租约上的key都失效，用于集群监控以及服务注册发现(回想etcd中watch命令的使用，可以监控key)

在windows下的etcd版本 

~~~c
打开windows PowerShell
.\etcdctl.exe txn -i	开启一个事务

~~~



mysql中MVCC 和 etcd中MVCC区别
mysql中通过undolog来实现回滚和MVCC，事务一旦提交， undolog就没啥用了， 而etcd中涉及到对以前版本的查找(多，而且需要通过key(rivision)去找到指定的版本)，所以使用B+树提高查找效率

raft算法细节：

~~~c
分为3个角色，  Candidate(候选人)， follow(选民)， leader(被选中的人)
    
只选举出一个leader

选举超时 计时开始情况
1.服务器启动的时候每个节点都会随机一个选举超时
2.当成为候选者时，会重新随机一个选举超时

每个人手里只有一个选票，收到其他的拉票并且自己没有投过票，就会投给第一个来拉票的候选人


什么时候，选举超时时间会重置？
刚开机时，
当收到别的candidate的拉票时，
当收到leader的心跳包时，
当选举时间超时成为candidate时，选举时间重置
(注意选举时间是随机的，一般在150ms ~ 300ms之间)
    
    
选举超时后，需要重置自身选票

如果有两个节点的的选举超时是一样的，并且出现平票的情况，
上边已经说了，当选举超时时间到了之后会重置选举超时时间，等待下一次重选就好了，注意下一次就不一定会选到谁了， 相当于此时所有的节点都在等待选举超时时间，当谁先选举时间超时，就去拉票， 可能会有疑惑已经没票了，注意看上边说的，选举超时后，重置自身选票。


选举成功后， 就是日志复制了。
当收到客户端的数据时，leader先记录到wal日志中，leader会告诉其他节点，询问是否认可，当leader收到超过一半同意时， leader会将数据写到磁盘，并且告诉所有节点这个数据，其余各节点也将该数据记录日志后，刷到磁盘， 实现同步


脑裂出现情况： 网络出现分段， 可能出现多个leader， 当有客户端发送数据来的时候， 需要发送给当前网络分段的所有其他节点， 比如一共有5个节点， 但由于网络分段， 导致当前网络段只有两个节点， 另外一个网络段有3个节点， 此时每个网络段各有一个leader， 如果此时有两个节点的leader收到客户端的数据， 它会询问另一个普通节点(这个网段只有两个节点， 一个leader 一个 普通)， 但此时就算这个普通节点同意， 加上leader自己的一票，  只有两票， 没有超过半数， 所以此次客户端的提交不会通过， 但如果是 3个节点的网络段收到数据， 一共能有3票支持，超过半数， 就会认可客户端的这个数据。
当网络恢复时，此时有两个leader， 但因为3个节点的那个网段的leader接受成功过客户端的数据， 所以它的rivision 会比两个节点中的leader的rivision大， 所以这个3个节点中的leader重新成为 这个五个节点中的leader。
~~~

## ZeroMQ

如何提高消息队列的效率： 1.批量发送(而不是一应一答)，2.增加车道(多配置几个主机)

内部已经解决了tcp粘包半包的问题

不是一个独立的服务， 需要嵌套到应用程序中去。

 ## 加密工具

md5的全称是md5信息摘要算法（英文：MD5 Message-Digest Algorithm ），一种被广泛使用的密码散列函数，可以产生一个128位（16字节，1字节8位）的散列值（常见的是用32位的16进制表示，比如：0caa3b23b8da53f9e4e041d95dc8fa2c），用于确保信息传输的完整一致。

md5 跟对称和非对称加密算法不一样， 这两种加密算法是防止信息被窃取， 而摘要算法的目标是用于证明原文的完整性

md5的用途

    密码的加密存储，用户设置密码时，服务端只记录这个密码的MD5，而不记录密码本身，以后验证用户身份时，只需要将用户输入的密码再次做一下MD5后，与记录的MD5作一个比较即可验证其密码的合法性。
    数字签名，比如发布一个程序，为了防止别人在你的程序里插入病毒或木马，你可以在发布这个程序的同时，公开这个程序文件的MD5码，这样别人只需要在任何地方下载这个程序后做一次MD5，然后跟公开的这个MD5作一个比较就知道这个程序是否被第三方修改过。
    文件完整性验证，比如当下载一个文件时，服务器返回的信息中包括这个文件的md5，在本地下载完毕时进行md5，将两个md5值进行比较，如果一致则说明文件完整没有丢包现象。
    文件上传，比如百度云实现的秒传，就是对比你上传的文件md5在百度服务器是否已经存在了。

## 分布式锁

redis， mysql，etcd， zookeeper	

与普通锁比起来，难点就是要处理网络，因为实例在不同机器上

需要实现：

1.互斥， 2 .锁超时性 3. 锁是一种资源， 需要具备可用性（CAP）， 4.容错性

只要满足高可用和强一致就能满足上诉的3 和 4

C ： 一致性(强一致性) ，和数据库中的一致性不是一个概念
A ：可用性
P：分区容错性

可用性分为：

计算型的可用性：开启多个功能相同的服务（无状态）

存储型的可用性：主从切换的机制



mysql中的分布式锁实现：



![1669899219150](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\1669899219150.png)

![1669899702405](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\1669899702405.png)

~~~mysql

select * from lock_t where uuid = xxx
lock in share mode //S锁
for update 		  // X锁
~~~

insert into ， update， set， 自动加X锁

1.S锁和X锁互斥实现 (X,X也行)

![1669900648717](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\1669900648717.png)

![1669900627778](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\1669900627778.png)



## HEX, BASE64, urlEncode

为什么要对url编码

基本的 ASCII 字符集共有 128 个字符，其中有 95 个可打印字符，包括常用的字母、数字、标点符号等，另外还有 33 个控制字符。标准 ASCII 码使用 7 个二进位对字符进行编码，对应的 ISO 标准为 ISO646 标准。

**1. ASCII 的控制字符**

这些字符都是不可打印的，自然需要进行转化。

**2. 一些非ASCII字符**

这些字符自然是非法的字符范围。转化也是理所当然的了。

**3. 一些保留字符**

很明显最常见的就是“&”了，这个如果出现在url中了，那你认为是url中的一个字符呢，还是特殊的参数分割用的呢？

**4. 就是一些不安全的字符了。**

例如：空格。为了防止引起歧义，需要被转化为“+”。

明白了这些，也就知道了为什么需要转化了，而转化的规则也是很简单的。

按照每个字符对应的字符编码，不是符合我们范围的，统统的转化为%的形式也就是了。自然也是16进制的形式。

5.比如一些数据传输不支持二进制的方式，比如url就要求是文本，所以这个时候就可以使用base64编码

## kafka

kafka是一个分布式，分区，多副本的提交日志服务。

kafka优势

```
1.吞吐量高， 性能好，
2.伸缩性好，支持在线水平拓展
3.容错性(比如同一份消息会存储三份，并且持久化到硬盘上)和可靠性(一主多从高可用架构)
4.与大数据生态紧密结合，可无缝对接hadoop，strom, spark, fink。 因为kafka在处理TB级别数据时不会出现明显的性能问题
```



kafka中有主题(可以理解为数据库中的表，通常将同一类型的消息存到同一个主题中，mysql中表的设计是完全结构化的，但kafka中是半结构化的，所以某些情况也可以将不同类型的消息存放到同一个主题中)，以及分区(使kafka具备了拓展性，分区是一个线性增长的不可变的日志，kafka会为每条消息分配一个偏移量offset， kafka通过这个偏移量来对消息进行提取，但没法对内容进行检索)的概念，一个主题有多个分区， 因为是分布式的， 所以这些同一个主题的分区可以分布在不同机器上。

kafka是以key-value 的形式存储的， 如果是具有相同key的消息， 会把这些信息放到同一个分区中，如果是空key，则轮询放入各个分区中，。

kafka保证了数据的可靠性，kafka通过replication-factor 这个参数来设置分区的副本数量

刚才说到了分区副本(副本不是放在同一台机器上的)，比如replication-factor=3(在这三个中选出一个当主本)，对数据的读写都在主本中(被成为leader)， 另外两个副本(follower)负责同步主本，kafka会在原数据中维护一个集合ISR，表示当前正常同步的副本，如果没有正常同步或者落后太多，则从ISR中移除，当追赶上来的时候再加进去，

kafka中有一个broker的概念， 被成为消息代理，比如对数据的读写请求就是由broker去处理的，一般就是一台机器上一个broker，以下图第一幅图为例，broker只负责处理p1分区的读写请求，不会直接处理p0和p2的读写请求，而只负责从p0和p2的主本(leader)中同步数据过来。 

![1677308857764](C:\Users\jiangtao5\AppData\Roaming\Typora\typora-user-images\1677308857764.png)



**搭建伪分布式集群**

[环境搭建-使用docker部署kafka_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1h94y1Q7Xg?p=7&spm_id_from=pageDriver&vd_source=6575af2bb3053be3df86d366bf9da1b6)

1.mkdir etc

2.cp config/zookeeper.properties etc  //它是zookeeper的配置文件

3.cp config/server.properties etc //它是用来配置kafka的配置文件， 由于我们需要3个broker实例，所以需要拷贝三份， 把这个命令修改为

~~~
cp config/server.properties etc/server_0.properties

cp config/server.properties etc/server_1.properties

cp config/server.properties etc/server_2.properties
~~~

4.进入这三个配置文件，分别把broker.id的值更改为0,1,2.  把listeners=PLAINTEXT://:9092中的 9092分别改为9092,9093,9094, 再把log.dirs=/tmp/kafka-logs中的logs分别改为logs-0, logs-1, logs-2

5.更改好之后去bin目录启动zookeeper， 执行
./zookeeper-server-start.sh ../etc/zookeeper.properties

~~~
启动zookeeper过程报错问题
如果java版本不支持
出现类似Kafka 无法识别的 VM 选项“PrintGCDateStamps”的报错

在bin/kafka-run-class.sh中把

JAVA_MAJOR_VERSION=$($JAVA -version 2>&1 | sed -E -n 's/.* version "([^.-]*).*"/\1/p')


换成

JAVA_MAJOR_VERSION=$($JAVA -version 2>&1 | sed -E -n 's/.* version "([^.-]*).*/\1/p')

区别就是少了一个"
~~~



6.接着启动三个kafka的实例
./kafka-server-start.sh ../etc/server-0.properties 
./kafka-server-start.sh ../etc/server-1.properties 
./kafka-server-start.sh ../etc/server-2.properties 

这三个配置文件中都有相同的配置
zookeeper.connect=localhost:2181

7.接下来是创建kafka的主题
./kafka-topics.sh --zookeeper localhost:2181 --create --topic test --partitions 3 --replication-factor 2

~~~
解释一下以上字段，
--zookeeper是必须的，因为zookeeper是在本地，所以写的是localhost，2181表示zookeeper监听的端口号 
--create表示要创建主题了，可以把它改为--describe表示查看主题的分区情况
--topic 表示要创建的主题名
--partitions 表示分区数量
--replication-factor 表示每个分区有多少份(主本+副本)
~~~

8.创建好了之后查看一下刚才创建的主题情况

~~~
./kafka-topics.sh --zookeeper localhost:2181 --describe --topic test

介绍一下主要字段：
partition表示分区号，
leader 表示主本在哪个服务器(broker)上
replicas 表示partition表示的这个分区在哪些服务器(broker)上
Isr 表示当前正常同步的服务器(broker)有哪些
~~~

9.kafka利用控制台模拟消费者消费数据

~~~
./kafka-console-consumer.sh --bootstrap-server localhost:9092, localhost:9093, localhost:9094 --topic test

此时已经在等待消费数据了，所以需要一个生产者
~~~

10.kafka利用控制台模拟生产者生产数据

~~~
./kafka-console-producer.sh --broker-list localhost:9092, localhost:9093, localhost:9094 --topic test  // 紧接着可以加个 --group表示消费组

在当前终端输入东西， 消费者那里就能接受到消息了。
~~~

**监听器 listeners 和 advertised.listeners以及内外网问题**

~~~
listeners: 指定broker启动时的本机监听端口，给服务端使用

advertised.listeners 对外发布的访问IP和端口，注册到zookeeper中，给客户端使用

流程是这样的，kafka把advertised.listeners注册到zookeeper中， 客户端从zookeeper得到这个地址， 从而能够访问到kafka，advertised.listeners配置的地址如果是同一个网段的情况下，则可以直接访问，不过不在同一个网段，则必须设置成公网地址才能访问

所以在实际的配置中需要给listeners配置一个外网的情况和一个内网的情况
所以在broker(server.properties)配置如下
listeners=INTERNAL:http://:9092,EXTERNAL:http://0.0.0.0:9093

advertised.listeners=INTERNAL:http://主机名(或者内网ip):9092,EXTERNAL:http://公网IP:9093

listeners.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT

//下边这行表示内部broker之间通信走的是内网，因为kafka集群一般都在同一个机房中
inter.broker.listener.name=INTERNAL

~~~

**消息模型**

~~~
分区是最小的并行单位
一个消费者可以消费多个分区
一个分区可以被多个消费组里的消费者消费
但是，一个分区不能同时被同一个消费组里的多个消费者消费
~~~



1.发布订阅模型

~~~
每个消费者都属于不同的消费组， 所以每个分区的每一个消息会被每一个消费者消费一次
~~~

2.点对点模型（一对一)

~~~
将所有消费者都放到同一个消费组里面
~~~

**消息顺序问题**

~~~
比如同一个生产者，生产了两条消息，但这个两个消息一个在partation0， 一个在partation1，kafka没办法保证顺序消费

解决办法：
1.只设置一个分区，这样就可以保证所有消息的顺序， 但是失去了拓展性和性能
2.设置相同的key，相同的key会发送到同一个分区， 同一个分区能够保证消费的顺序。
~~~

**消息传递**

~~~
最多一次
对于生产者，如果发送给broker的过程失败了，但没有重传机制，则最多发送一次

对于消费者， consumer先提交消费位置，再消费数据


最少一次
对于生产者，如果brker收到了，但回复消息给生产者的过程，这个回应丢失了， 生产者会再次发送

对于消费者， consumer先消费数据，再提交消费后的偏移量


精确一次
对于生产者：
enable.idempotence=true
retries=Integer.MAX_VALUE
Acts=all //0表示只通知broker生产了消息，不管成功与否，1表示broker中的leader收到了，但还未将数据同步给follower, all表示leader收到并且已经同步给follower

对于消费者
通过offset来防止重复消费不是一个好的办法
通常在消息中加入唯一ID（例如流水ID, 订单ID），在处理业务时，通过判断ID来防止重复处理
~~~

**事务**

~~~
lsolation_level  隔离级别
默认为: read_uncommitted 脏读
read_committed 读取成功提交的数据， 不会脏读
~~~



## zookeeper

总结：zookeeper主要就是通过znode的节点类型 + 监听机制 来实现很多实用的功能。

因为kafka用到了zookeper，简单介绍一下，可以用它来做统一配置管理、统一命名服务、分布式锁、集群管理。
zookeper的节点成为znode

znode有两种类型: 短暂(当客户端和服务端断开连接后，所创建的znode会自动删除)， 持久(连接断开后也不会删除。) ， 它们有一个共同特点，可以把节点的名字弄成顺序的(与做分布式锁有关)

zookeeper和redis一样都是C/S架构(分客户端和服务端)

理解了zookeeper的**结构**之后，还需要知道zookeeper需要配合**监听器**才能做这么多事，常见的监听场景有一下两种： 1.监听znode节点的数据变化 2.监听子节点的增减变化。

一、用zookeeper做统一配置管理

```
比如把我的IM做集群，每个机器上都有相同的配置文件，如果需要修改，那么就需要在每个机器上都进行修改， 所以可以把这些公共的配置文件放到zookeeper进行管理，同时会落地数据库， 同时会对应用开启配置实时监听，如果zookeeper配置文件一旦被修改，应用就可以实时监听到并获取。
```

二、用zookeeper做统一命名服务，理解上和域名一样，我们给一部分资源(多个ip地址)起一个名字，把这个名字挂到znode节点上 

三、用zookeeper做分布式锁

![img](https://pic3.zhimg.com/80/v2-338b221850de334723018c9164804576_1440w.webp)

```
举个例子：

系统A拿到/locks节点下的所有子节点，经过比较，发现自己(id_000000)，是所有子节点最小的。所以得到锁
系统B拿到/locks节点下的所有子节点，经过比较，发现自己(id_000002)，不是所有子节点最小的。所以监听比自己小1的节点id_000001的状态
系统C拿到/locks节点下的所有子节点，经过比较，发现自己(id_000001)，不是所有子节点最小的。所以监听比自己小1的节点id_000000的状态
```

四、用zookeeper来管理集群

![img](https://pic1.zhimg.com/80/v2-64f633e7f829b5daeedf5e4d116972bc_1440w.webp)

在zookeeper中创建一个groupMember节点，同时创建3个子节点表示三台不同机器上的服务，如果谁挂了，另外两台就可以感知到。

zookeeper可以实现动态选举master的功能，对于主从的选择， 可以把代表不同机器的znode节点弄成带顺序号的临时节点，zookeeper每次选举最小编号的znode对应的机器作为master，如果master挂了，对应的znode就会删除，然后让新的最小编号的znode对应的机器做master。

## Hbase

跟mysql的区别，mysql是行存储，hbase是列存储，mysql底层的文件系统就是我们一般的普通文件系统来进行数据存储， 而hbase的底层文件系统用到是HDFS，也就是分布式文件系统

从列的角度看， hbase有列族的概念(每个列族由多个列组成，列就是字段，比如name, age), 有一个特殊的列Row_Key(类似于mysql中的主键)

从行的角度看， 可以把一张表水平分为多个部分， 每个部分就是一个Region， 每个Region中的不同列族都是一个store, 我们实际存储就是以store为单位存储的

每个store**以行为单位进行列式存储**，这也就是为什么我们说hbase是列式存储的，以及为什么hbase不像mysql中那样，无法应对字段变更的场景，hbase能够轻松应对

hbase中的Name Space类似于关系型数据库中的database概念，hbase中的table类似关系型数据库中的表概念，不同的是，hbase定义表的时候只需要声明列组即可，不需要声明具体的列(name, age), 比如 声明一个列族info, 而name，age这些字段就相当于属于这个列族。 hbase中的每个列都由列族(info)和列限定符(name，age),进行限定，例如info : name, info : age ，建表时，只需要指明列族，而列限定符无需预先定义

hmaster 是hbase集群中的主服务器， 负责监控集群中的所有regionserver， 并且是所有元数据更改的接口，集群中可以启动多个hmaster但zk选举机制只能保证同时只有一个hmaster处于活动状态，其他hmaster处于热备份状态

hmaster主要负责表和region的管理工作

~~~
1.管理用户对表的增删查改操作，但其实这并不是hmaster特有的，所有的hbase都有
2.管理regionserver的负载均衡，调整region的分布
3.region的分配和移除
4.处理regionserver的故障转移，在RegionServer死亡后，负责对regionServer上的region进行迁移
~~~

当hmaster节点发生故障时，由于客户端是直接于regionserver交互的，且元数据表(meta)也是存在于zookeeper中的，所以整个集群依然会正常工作，但hmaster故障时，region的切片和regionserver的故障转义等问题就没法即使处理了，还是需要尽快恢复，zk提供了多hmaster机制，提高了hbase的可用性和稳健性

hmaster是如何感知regionserver故障的？
每个regionserver都会注册到zk上，regionserver会不断跟zk发送心跳，当超时后，zk会删除整个regionserver的节点，hmaster监听了这个事件，于是就可以开始进行容错工作了

云数据表存放哪些信息？
rowkey : 有四个字段拼接起来，分别是 表名-StratRow-TimeStamp-EncodedName。
数据分为4列
info: regioninfo:EncodedName、RegionName、Region的StartRow、Region的StopRow
info: seqnumDuringOpen: 存储Region打开时的sequenceId
info: server : 存储region落在哪个regionserver上
info: serverstartcode: 存储所在的regionserver启动时间戳

元数据表只有一个列族， 即info ，并且，hbase保证meta表始终**只有一个region**，这是为了保证meta多次操作的原子性(对表操作的时候会加分布式锁的，这个分布式锁是通过zookeeper实现的)， 客户端一般会把meta表的region信息缓存到hbase客户端， hbase客户端有一个叫metacache的换成，在调用API时，客户端会先去metacache中找到业务rowkey所在的region

## HDFS

查看

~~~
hadoop fs -ls 
~~~

创建目录

~~~
hadoop fs -mkdir -p /user/hadoop/cmd

-p 表示递归
~~~

上传本地文件到hadoop

~~~
hadoop fs -copyFromLocal word.txt /user/hadoop/cmd

如果上传一样的文件到相同的路径需要加 -f 表示覆盖
把-copyFromLocal换成 -put 也可以上传
~~~

查看文件内容

~~~
hadoop fs -cat /user/hadoop/cmd/word.txt
~~~

不指定上传某个文件到hadoop，而是直接用输入流的方式

~~~
hadoop fs -put - /user/hadoop/cmd/put.txt

接下来自己随意输入内容，以ctrl + d 结束输入， 此时hadoop上就会有一个put.txt文件，里面就是我们刚才输入的东西。
~~~

把hadoop上的文件下载到本地

~~~
hadoop fs -get /user/hadoop/cmd/put.txt
~~~

hadoop创建文件

~~~
hadoop fs -touchz /user/hadoop/cmd/flag.txt
~~~

hadoop移动文件

~~~
hadoop fs -mv /user/hadoop/cmd/flag.txt /user/hadoop/flag.txt
~~~

hadoop修改某个文件的权限

~~~
hadoop fs -chmod 777 /user/hadoop/cmd/put.txt
~~~

hadoop修改某个文件夹的权限

~~~
hadoop fs -chmod 777 -R /user/hadoop/cmd
~~~

hadoop修改文件内容

~~~
hadoop dfs -get yourHdfsPath/test.txt
vi test.txt #or use any other tool and modify it
hdfs dfs -put -f test.txt yourHdfsPath/test.txt
~~~

hadoop查看某个文件是否存在

~~~
hadoop dfs -test -e file //如果存在返回0
~~~





补充：hdfs和Linux下文件系统一样都是树的形式，hadoop fs 和 fadoop dfs 都是一样的效果



## docker

**docker一直starting** 

```csharp
net stop com.docker.service
net start com.docker.service
cd "C:\Program Files\Docker\Docker"
./DockerCli.exe -SwitchDaemon

都不行
wsl --update
```

docker启动不了服务

```
 docker run --name xx --entrypoint /bin/bash -dit db_proxy_server:latest
 直接进入服务内部
```

## wsl

安装wsl命令

~~~
wsl --install
~~~



查看当前wsl版本

~~~
wsl -l -v
~~~

查看可下载的wsl

~~~
wsl -l -o
~~~

比如下载Ubuntu-20.04

~~~
wsl --install -d Ubuntu-20.04
~~~



升级 wsl1->wsl2

~~~
wsl --set-version Ubuntu-20.04 2
~~~



