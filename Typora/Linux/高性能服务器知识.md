## 负载均衡器

~~~shell
负载均衡器有自己的ip，一般是用域名，然后域名用负载均衡器的ip，有了负载均衡器，就可以避免党提供服务的机器变化时，ip跟着变化，需要修改域名映射的ip等操作，并且能监控节点活性进行合理选择等，一般设计中：前端就用这个域名:端口访问负载均衡器，然后负载均衡器有个监听器的概念，分为http/https类型和tcp/udp类型，

1.http类型的监听器：一般需要配置一个域名(test.com)加url路径(一般就用/，所有请求默认都走/，除非有更细致的url路径规则如/test/jtt/)，以及监听类型和端口如https:9030，此时收到https://test.com:9030，就会来到被这个监听器捕获，可以对这个监听器配置一组后端服务器，把流量转发到这组后端服务器的哪个端口也是可以配置的不一定是9030，但本质上这组后段服务器功能是一致的，所以一般保持一样用9030就行，避免记混。
2.tcp类型的监听器：tcp长连接的网络开发(客户端conn并且保留socket进行后续socket通信，可以理解为玩家的连接服)就是走的这种。

会话保持方式：
1. http监听器:基于cookie(在请求头中)，由应用程序生成(如webpy)或者负载均衡器生成
2. tcp监听器:基于源ip

tcp长连接：
因为一条长连接中客户端每次发送的数据四元组（源 IP、源端口、目标 IP、目标端口）都是一样的，负载均衡器会记录这个四元组的映射关系，来保证长连接的每次请求都去同一个后端服务器，当该tcp断开时会发送FIN包，负载均衡判断到这个包就会删除映射关系。

# 拓展，开发环境可以有很多个不同游戏的域名绑定同一个负载均衡器的ip，然后针对不同游戏设计各自的监听器规则，比如游戏A是https://youxia.com:8030，游戏B是https://youxib.com:9030，域名youxib.com和youxia.com都用同一个负载均衡器的ip
~~~

### 阿里云CLB

~~~shell
tcp监听器和http监听器得分开讨论。

tcp监听器（工作于第四层传输层）：
一句话总结：四元组->ecs的映射保证了单条连接永远在同一个ecs处理，tcpdump抓包看到的是虽然是客户端真实ip，但回复数据时ecs内核有插件(类似toa)会把包的目的ip更改为clb的ip，所以回复的数据会先去clb，心跳检测连接后立马断开。
调度策略：1.权重、2.轮询、3.hash一致性（基于源ip或基于四元组）
会话保持：只基于源ip，并且是在调度策略之后的，比如出自同一公网ip的client1和client2，client1先请求连接先根据调度策略被分配到ecs1上，如果开启了会话保持client2也会被分配到ecs1(因为client1已经建立了ip和ecs1的映射)，如果两个client都断开了(映射信息就不存在了)，下次client1/2重新请求的时候会重新根据调度策略选择一个ecs。
调度策略和会话保持是不冲突的，会话保持说的是已经有了某个源ip->ecs的映射后，之后相同的ip的数据都会去同一个ecs，而调度策略是在一开始的时候决定某个连接请求该被分配到哪个ecs上，如果调度策略选择的是hash一致性基于源ip那就确实没必要开启会话保持功能。

5.clb会定时给ecs发送tcp连接再立马断开连接，检查ecs的活性
4.hash一致性（基于四元组）如果开启，当源ip:源port始终不变时(前端bind了固定端口)，那么永远会去同一个ecs(第一个数据包建立四元组->ecs的映射时，选择ecs的调度规则就是通过源ip:源port进行hash)，但如果没开启，当长连接断开时，尽管源ip:源port一样，也会重新根据别的调度策略选择ecs。
3.包从客户端发到clb时的判断处理，先判断是否有会话保持(源ip->ecs)，再判断是否已经存在四元组->ecs的映射，如果都没有就根据调度策略选择一个ecs并建立四元组->ecs映射。
2.clb扮演的角色只是修改数据包的源ip:源port和目的ip:目的port，ecs上看到的连接四元组为(客户端真实ip:port<->ecs的ip:port)，但ecs机器都内置了类似于toa插件的东西(可以理解为帮助找到clb的ip:port)，所以ecs回复的时候是会将消息先发到clb的，尽管通过tcpdump抓包看到的数据全是真实客户端ip:port与ecs的交互，没有clb地址的信息，其实底层包的转发逻辑都由ecs内置的插件处理了(可以理解为插件在tcpdump的下一层又把目的ip:port改成了clb的)，也就是说数据包的发送和回复都会经过clb，但对使用者来说是透明的，在服务器和客户端都察觉不到clb的存在。
1.客户端调用conn请求连接，tcp的三次握手过程会跟同一个ecs打交道，第一个数据包过来的时候，建立了四元组->ecs的关系，ecs的选择是根据调度策略(1.权重、2.轮询、3.hash一致性（基于源ip或基于四元组）)计算出来的，之后这个长连接都会走这个ecs


http监听器（工作于第七层应用层）：
一句话总结：客户端与clb建立长连接，clb不与ecs建立长连接，相当于Nginx反向代理(腾讯云的不是)，ecs用tcpdump抓包看到的源ip不是真实客户端ip,通过http请求的head中的X-Forwarded-For字段获取真实客户端ip，心跳检测用HEAD方法走/路径。
调度策略：1.权重、2.轮询   # 不支持四元组，因为在第七层
会话保持：1.只基于cookie，但cookie的产生可以有两种选择，第一种是由clb生成(植入cookie)，第一次http访问之后客户端会纪录下这个cookie，之后的http请求都会带着这个cookie，cookie过期时间可以配置，第二种是由自己的后端应用生成(重写cookie)，需要在clb配置cookie的名字，比如应用生成的cookie为"shengji_session=126f45d5396cd3d88d62919621c978d7",就需要在clb填shengji_session,过期时间由后端应用决定，总之就是clb检测到http的返回中包含cookie时则会建立cookie到ecs的映射，光是在请求中加上cookie，但http的返回中没有cookie字段是无法建立映射的。

5.帮助人员说可以把http监听器理解为更高级的Nginx反向代理
4.tcpdump抓ecs的包看到的源ip不是真实客户端的，也不是clb的，而是阿里云内部的100.开头的机器，这一点跟tcp监听器是不一样的，tcp监听器抓包可以直接看到源ip信息
3.客户端发的是keep-alive，但clb发给服务器的时候就变成close了，原因是客户端只能和clb建立了长连接，但clb和ecs不是长连接，clb回复给前端的时候还是keep-alive。(腾讯云抓包看到的是客户端真实ip，虽然clb发送给ecs的时候还是keep-alive，但也没有跟ecs建立长连接,ss -tap | grep port可以看到没有长连接信息)
2.clb会定时给ecs发送http连接，默认是http://ip:port/(默认/路径)，检查ecs的活性
1.可以从head中的‘x-forwarded-for’看到源ip
~~~

### SSL/TLS终止

~~~
前端发送的是https请求，负载均衡解密SSL/TLS加密的数据，只涉及去除加密层，不修改http请求的内容本身，将解密后的http请求再转发给后端服务器，可以减轻后端服务器负担不需要处理加密解密
~~~

### k8s粘性会话

~~~shell
2.tcp长连接天然支持粘性会话(负载均衡器和k8s服务会维护一个连接表(四元组映射路由信息)，udp因为不是面向连接的，所以没有这个表)，玩家与k8s集群中的某个pod建立tcp长连接后，下次该玩家也一定会去这个pod，粘性会话主要用于非长连接协议，比如http1.0(尽管http基于tcp，但不开启持久性会话就不是长连接，每次请求执行完就断开连接了)，udp等基于他们的协议，并且希望玩家始终去同一个服务上，就需要开启会话保持功能。
1.负载均衡器只是到达k8s的svc层面，而svc选择哪个pod是k8s内部的机制，可以在service配置中加上sessionAffinity: ClientIP来实现粘性会话，相同的客户端ip会去同一个pod，长连接不需要考虑这个设计，短连接服务，并且要求去同一个pod就有用了
~~~

### 负载均衡器数据流向

~~~
数据流向: 前端->负载均衡器(修改数据包的源ip和源port，改为负载均衡器自己的ip和新产生的不冲突的port，这样才能有唯一的四元组，所以负载均衡器受到port数量限制，收到后端服务器返回时再根据四元组映射的客户端信息进行目的ip和目的port的修改，源ip，源port修改为是客户端发过来时的目的ip和目的port，其实也就是负载均衡的ip以及监听器监听的port)->后端服务器->负载均衡器->前端。

客户端到服务器的映射表：(长连接用，短连接建立时可能会有数据，但连接断开时就移除了)
键：(客户端 IP, 客户端端口, 负载均衡器 IP, 负载均衡器监听端口)
值：(服务器 IP, 服务器端口)
这个表用于处理客户端的请求，找到相应的后端服务器，也是长连接一定去同一个服务的原因

服务器到客户端的映射表：
键：(负载均衡器 IP, 负载均衡器随机生成的端口, 服务器 IP, 服务器端口)
值：(客户端 IP, 客户端端口)
这个表用于处理从服务器返回的数据包，确保能正确转发给客户端
~~~



## 统一事件源

### 前置知识

信号处理机制

~~~
信号会中断当前正在执行的线程(多线程下，优先主线程处理，如果第二个信号来临时，主线程还没处理完则由其他线程处理第二个信号)，比如3个线程(包含主)，注册了SIGINT信号，前三次control+c都会有线程回应，第四次的表现是程序卡住，等到有空闲线程释放出来才处理第四个信号。
~~~

信号的处理建议用统一事件源管理

~~~c++
如果不用统一事件源的问题:
1.不安全，比如业务逻辑获取了一把锁，此时信号的回调也要这把锁则死锁
2.不可重入问题，比如test()是不可重入函数，执行到一半被信号中断，中断处理再次调用了test()
3.简化了逻辑，更容易管理事件。
~~~

重入函数细节

~~~c++
1.“可重入函数”定义:某函数执行过程中被中断，中断处理中再次调用此函数，当中断恢复时，该函数恢复执行，并且结果不受到刚才的中断影响。跟多线程调用同一个函数没有关系，要分开讨论，完全不是一个概念。
2.可重入函数未必是线程安全的函数
//+、-都不是原子操作，多线程下会计算有误差；但它是可重入函数，因为就算被中断，他最终的计算结果也是预期的10，所以就算用了全局变量，也可以是一个可重入函数
int count = 10;
void test() {
    count++;
    count--;
}
3.线程安全的函数未必是可重入函数(比如函数中用了锁保证线程安全，但中断时再次尝试获取锁可能死锁)
4.既是可重入函数又是线程安全函数
//errno是errno.h的一个全局变量，他是线程局部存储的(TLS，每个线程有各自的errno变量，所以是线程安全的)，同时又保证了可重入性。
void sig_handler(int sig)
{
      int save_error = errno;
      int msg = sig;
      send(pipefd[1], (char*)&msg, 1, 0);
      errno = save_erro;
}
5.可重入函数内不能使用不安全的函数，比如printf()，他的底层用了malloc，free，它们操作了全局链表来管理内存，如果执行过程中被中断，可能会带来预期外的结果。
node_t node1, node2, *head;
void insert(node_t* p) {
  p->next = head;
  // 如果在这被中断了，中断执行时插入的那个节点就没人指向它了，再也访问不到，出现内存泄漏问题，malloc同理。
  head = p;
}
~~~

可重入函数举例

~~~c++
void sig_handler(int sig)
{
      int save_error = errno;
      int msg = sig;
      send(pipefd[1], (char*)&msg, 1, 0);
      errno = save_erro;
}
// 异步信号安全的函数，如 write, read, send, recv 等。
// 开始执行前记录了原始errno值，最后进行了恢复

int count = 10
void test()
{
  count ++;
  count --;
}
~~~

不可重入函数举例

~~~c++
void test() {
    static int count = 0; // 使用静态变量，可能导致数据竞争
    count++;
    printf("Count: %d\n", count);
}
// 1.printf 不是一个异步信号安全的函数。它可能会在内部调用诸如 malloc、free 等函数，这些函数在信号处理器中调用时可能会导致数据竞争和不可预期的行为，例如，printf 可能会修改标准输出缓冲区的状态，而这个缓冲区可能在主程序或其他线程中也在使用。
// 2.不要用全局或静态变量
~~~

## 多reactor模型设计

### 前置知识

1.如果B进程继承了A进程的listenfd(也就是说先创建listenfdfork出来B进程)，虽然A、B进程有独立的文件描述符表，但listenfd这一表项指向的内核资源是同一个

2.(如果fork之前执行epoll_create)也是和第一点同样的道理，进程的底层内核资源epoll是同一个。

3.epoll和listenfd都是文件描述符，并且它们都是内核资源(也就是说fork出来的进程尽管在用户态有独立的地址空间，但内核资源都是同一份)，epoll文件描述符比较特殊专用事件通知机制。

### 情景分析

#### 第一种（非多reactor，无惊群）

~~~shell
执行顺序bind->listen->epoll_create->fork(listenfd需要在fork之前加入epoll，因为父子进程的listenfd和epoll的底层资源是同一个，这种情况创建的listenfd无法在fork之后多次添加到epoll中，所以必须在fork之前)，并不是多reactor模型，也没有惊群现象 # 我是这么理解它没有惊群现象的(所有进程共用同一个epoll(内核资源层面)，也就是说他们底层是同一个双向链表，内核使用锁机制确保对双向链表(共享资源)的同步访问，所以不会有惊群问题。
~~~

情景演示：child进程接收了新连接fd为5，但断开连接(ctrl+c)的时候却是parent进程处理的，两个进程虽然有独立的资源映射表，但指向的内核资源是同一个

![1718259755326](../../pic\1718259755326.png)

#### 第二种（多reactor，但有惊群现象）

~~~shell
只设置SO_REUSEADDR不设置SO_REUSEPORT，先listen，接着bind，fork之后再逐个进程执行epoll_create，然后在每个进程中都把同一个listenfd放进去，那么这个时候就有惊群现象因为它们的监听队列是同一个，其中一个进程能成功accept，其余的会返回-1. 此时就算设置SO_REUSEPORT也不能避免惊群现象，其实这里提到SO_REUSEPORT只是幌子，它的作用只是能对同一个ip+port重复调用bind函数，因为有地方说SO_REUSEPORT使得多个进程有独立的监听队列，但跟这里没关系，当前说的这种情况下本质上只执行了一次bind，所以只有一个监听队列，是否用SO_REUSEPORT都没有影响。
#我是这么理解它的惊群现象的，此情况下内核创建了两个epoll资源，也就是两个双向链表，epoll底层的触发机制是，如果有事件则会通知监听这个事件的所有进程。

#如果第一个进程处理的时机较早(调度顺序进程取决于内核无需关心)直接就把listenfd触发的事件处理完了，当第二个进程被唤醒的时候，去查看自己的双向队列发现没有事件了(因为两个进程的listenfd的底层内核资源是同一个，其中一个进程处理了，第二个进程就自然看不到了)，于是就会继续阻塞(epoll_wait的设计本就是没有事件的时候就等待，所以不用纠结为什么被唤醒了却没有直接返回0)，所以这个惊群现象并不是必现的，取决于进程尝试去处理事件的时机)
~~~

#### 第三种（多reactor，无惊群）

~~~shell
给listenfd注册EPOLLEXCLUSIVE事件，能保证只有一个进程被唤醒去处理连接请求(理论上不设置这个标志则事件触发时会尝试通知所有关注这个事件的进程)，但这样的问题就是连接的效率被限制了，同一个时刻只有一个进程在处理连接，亲测连接请求分布不均匀，可能一直是一个进程在处理
epoll_event event;
event.data.fd = listenfd;
event.events = EPOLLIN | EPOLLET;
event.events |= EPOLLEXCLUSIVE
~~~

#### 第四种（多reactor，无惊群，多监听队列）

~~~shell
使用reuseport的特性，这次一开始就直接fork进程，然后在每个进程中分别按序执行bind,listen,epoll_create，再把这个listenfd加入到epoll中，此时执ss -ltn 可以看到创建了几个进程就有几个监听同一端口的套接字。
~~~

####  拓展：前提是不用epoll，只有accept

~~~shell
(根本不涉及nginx的accept锁的概念，但如果引入了epoll，nginx网络模型的accept锁是必须的)，比如bind后进行fork，每个进程(包含父)都同时while(1)只调用accept函数，操作系统保证只有一个进程去accept，其余的进程都不会有惊群现象，也就是说其余的进程并不会出现accept失败继续执行之后代码的情况 # 我是这样理解的，跟第一种类似，所有进程在内核层面共用了同一个监听队列，内核使用锁机制确保对监听队列(共享资源)的同步访问，所以不会有惊群问题, 我猜测比如第一个进程成功accept，第二个进程同样会被触发，只是检测队列的时候发现为空所以继续阻塞等待，跟第二种没发生惊群保持阻塞一样的原理
~~~

SO_REUSEPORT和SO_REUSEADDR分析

~~~shell
SO_REUSEADDR: 不允许同一个端口被重复bind，解决的问题只有一个，比如一个进程杀掉之后会进入TIME_WAIT状态，此时重启这个进程会报错Address already in use, 原因是因为新程序如果直接用这个端口可能收到旧的数据包（比如四次挥手的时候失败了重传第三次挥手），使用这个标志可以避免这个报错，但如果用的不合理的话还是有收到旧数据的可能。
应用场景：同一个相同功能的进程，想要快速重启，避免等待

SO_REUSEPORT: 允许同一个端口被重复bind，它其实已经包含了SO_REUSEADDR的功能，现代编程中两个标志都用的原因是为了增强程序的兼容性和可移植性。(因为有些系统可能要求必须用SO_REUSEADDR)

场景举例：
如果是先bind了，再fork进程(本质上没有再次bind的步骤，都是直接继承的父进程)，不管是否使用SO_REUSEPORT，所有进程都会共用同一个监听队列(通过ss -ltn可以看到只有一个监听套接字)，用SO_REUSEPORT的好处是，如果多次调用bind可以成功，比如同一个进程，你可以开三个相同的实例绑定在同一个端口，那么这个时候同一个端口就有三个监听队列，由操作系统进行负载均衡决定一个新的连接放到哪个监听队列中，可以使用telnet ip port/nc ip port来模拟tcp的连接请求测试服务。
~~~

