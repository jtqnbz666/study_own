电脑开机密码：TPDue4S7xKbNp3!

新开机密码: Nswdsm58@wh010921

登录服务器密码: Nswdsm58@wh

极库云git代码，https 密码： Nswdsm58_

计算平台创建文件title_vector_put1/title_vector_put/vector2hbase/vertical_data_transfer/testwenku_fiel	TPDue4S7xKbNp3!ds.ini

dev103.se.corp.qihoo.net

dev106.se.corp.qihoo.net

offline01.qsst.zzzc.qihoo.net			sudo -u eng -H bash

docker14v.search.corp.qihoo.net   //go

sudo /usr/local/go1.16/go/bin/go env -w GOPATH="/home/jiangtao5/"



21教育，齐齐文库，差字典
(site == "https://wendang.chazidian.com/" or site == "www.21cnjy.com" or site == "www.qiqiwenku.com")): 爱问的数据是以byte为单位的，其他是KB为单位



验证数据是否成功入hbase

hbase2 shell   	get 'library_document_down_file','md5'

求md5 和 sha1

~~~ |
echo -n "xx" | md5sum
echo -n "xx" | sha1sum
~~~

小Q文档；./qbus_write_robin_file part-00000 shbt_priv_scp_2 wenku_xqppt_download

巨坑， 要把自己用户的文件放到eng下对应的自己的账户下才可以上传文件到hadoop

问答数据源头：
/home/eng/huangjianfeng/wenda/index/20230303/merge_fresh_inc_data_and_split_bucket_step_all

**问答数据存储：**hdfs://namenodefd4v.qss.zzzc.qihoo.net:9000/home/eng/jiangtao5/wenda

智选数据源头：
/home/qss/data/qss_engine_log/zhixuan_data_index/

**智选数据存储**
hdfs://namenodefd4v.qss.zzzc.qihoo.net:9000/home/eng/jiangtao5/wenda



查看集群配置

![1678174796841](C:\Users\jiangtao5\AppData\Roaming\Typora\typora-user-images\1678174796841.png)



测试flink

先拉一份数据过来

fresh数据
/home/eng/makaiquan/wenda/fresh/ 
hadoop dfs -get /home/eng/makaiquan/wenda/fresh/20230228/part-0-2

inc数据
/home/eng/makaiquan/wenda/inc/ 
hadoop dfs -ls /home/eng/makaiquan/wenda/inc/20230215/part-0-99

 export LD_LIBRARY_PATH=.:$LD_LIBRARY_PATH

~~~
生产者 
 
 fresh 流程
  ./qbus_write_robin_file ./test.txt zzzc_priv_qssl2eng1 zzzc_priv_qssl2eng_common_test2
 
 inc流程
 ./qbus_write_robin_file ./test-inc.txt shbt_priv_scp_2 shbt_priv_scp_2_comon_test2
 
 控制台
 ./kafka-console-producer.sh --broker-list
  --topic zzzc_priv_qssl2eng_comon_test2
 
 
消费者


fresh流程

./kafka-console-consumer.sh --bootstrap-server 10.172.95.63:39092 --topic 
zzzc_priv_qssl2eng_vertical_wenda_fresh_output_test  --group wenda-jt

inc流程
./kafka-console-consumer.sh --bootstrap-server  10.217.116.31:39092 --topic shbt_priv_scp_2_wenda_inc_sink --group wenda-jt


./kafka-console-consumer.sh --bootstrap-server 10.173.92.202:2181,10.173.92.203:2181,10.173.92.218:2181  --topic 
~~~

配置文件信息：hadoop dfs -cat /home/maintable/vertical/config/wenda-test.json

![1678241841281](C:\Users\jiangtao5\AppData\Roaming\Typora\typora-user-images\1678241841281.png)

集群：zzzc_priv_qssl2eng1

ip：10.172.95.63:39092

**对于fresh流**

生产主题 ;
zzzc_priv_qssl2eng_common_test2
消费主题;
zzzc_priv_qssl2eng_vertical_wenda_fresh_output_test

**对于inc流**

生产主题
shbt_priv_scp_2_comon_test2
消费主题
shbt_priv_scp_2_wenda_inc_sink





inc 和 fresh 数据位置
/home/eng/makaiquan/wenda/fresh/   fresh每天的数据 hadoop上
/home/eng/makaiquan/wenda/inc/       inc 每天的数据



```
zzzc_priv_qssl2eng", "10.173.92.202:2181,10.173.92.203:2181,10.173.92.218:2181
```





申请代码权限

https://cas.src.corp.qihoo.net/index.php/Auth/Permission/addIndex

wiki权限找运维

flink流

[qdev - Revision 285538: /qsearch/branches/flink_stream (qihoo.net)](https://se.src.corp.qihoo.net/qdev/qsearch/branches/flink_stream/)





wiki (资料)

https://wiki.so.qihoo.net/pages/viewpage.action?pageId=137907748



hulk云平台(申请服务器)

https://hulk.qihoo.net/user/host/list#
密码: Nswdsm58@wh





hadoop dfs -ls /home/maintable/vertical/config/



运行flink， hulk中点击工作台，任务开发，实时任务

奇麟大数据

http://qilin.qihoo.net/#/scheduler/projects/query?type=create



查看已经申请的代码

[qdev - Revision 285538: /qsearch/branches/flink_stream (qihoo.net)](https://se.src.corp.qihoo.net/qdev/qsearch/branches/flink_stream/)

密码：nswdsm58





安装mvn

链接：https://pan.baidu.com/s/12SuxtOXiUhNgkb0DH1eyCw
提取码：gnaf

配置bin环境

使用mvn管理jar包

pom.xml配置

```
方式一：

mvn install:install-file -Dfile=E:\workspace\vertical_wenda\lib/flink-connector-kafka-0.9_2.11-1.11.1-f2-qihoo.jar -DgroupId=org.apache.flink -DartifactId=flink-connector-kafka-0.9_2.11 -Dversion=1.0.0 -Dpackaging=jar

<groupId>flink-connector-kafka</groupId><artifactId>flink-connector-kafka</artifactId><version>1.0.0</version>


import sahded.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer09;

方式二：
把09kafka版本改为1.9.2
```



-Dfile : 源jar路径

-DgroupId : 相当于在C:\Users\jiangtao5\.m2\repository之后的路径

-DartifactId : 相当于-DgroupId之后的路径

这些值可以在flink中的pom.xml文件中看到





消费kafka数据

~~~
 ./kafka-console-consumer.sh --bootst
rap-server 10.173.194.164:39092  --topic zzzc_priv_qssl2eng_vertical_offl
ine_wenda_fresh  --group wenda-jt-test
~~~

