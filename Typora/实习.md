

查看集群配置

![1678174796841](C:\Users\jiangtao5\AppData\Roaming\Typora\typora-user-images\1678174796841.png)



测试flink

先拉一份数据过来

fresh数据
/home/eng/makaiquan/wenda/fresh/ 
hadoop dfs -get /home/eng/makaiquan/wenda/fresh/20230228/part-0-2

inc数据
/home/eng/makaiquan/wenda/inc/ 
hadoop dfs -ls /home/eng/makaiquan/wenda/inc/20230215/part-0-99

 export LD_LIBRARY_PATH=.:$LD_LIBRARY_PATH

~~~
生产者 
 
 fresh 流程
  ./qbus_write_robin_file ./test.txt zzzc_priv_qssl2eng1 zzzc_priv_qssl2eng_common_test2
 
 inc流程
 ./qbus_write_robin_file ./test-inc.txt shbt_priv_scp_2 shbt_priv_scp_2_comon_test2
 
 控制台
 ./kafka-console-producer.sh --broker-list
  --topic zzzc_priv_qssl2eng_comon_test2
 
 
消费者


fresh流程

./kafka-console-consumer.sh --bootstrap-server 10.172.95.63:39092 --topic 
zzzc_priv_qssl2eng_vertical_wenda_fresh_output_test  --group wenda-jt

inc流程
./kafka-console-consumer.sh --bootstrap-server  10.217.116.31:39092 --topic shbt_priv_scp_2_wenda_inc_sink --group wenda-jt


./kafka-console-consumer.sh --bootstrap-server 10.173.92.202:2181,10.173.92.203:2181,10.173.92.218:2181  --topic 
~~~

配置文件信息：hadoop dfs -cat /home/maintable/vertical/config/wenda-test.json

![1678241841281](C:\Users\jiangtao5\AppData\Roaming\Typora\typora-user-images\1678241841281.png)

集群：zzzc_priv_qssl2eng1

ip：10.172.95.63:39092

**对于fresh流**

生产主题 ;
zzzc_priv_qssl2eng_common_test2
消费主题;
zzzc_priv_qssl2eng_vertical_wenda_fresh_output_test

**对于inc流**

生产主题
shbt_priv_scp_2_comon_test2
消费主题
shbt_priv_scp_2_wenda_inc_sink





inc 和 fresh 数据位置
/home/eng/makaiquan/wenda/fresh/   fresh每天的数据 hadoop上
/home/eng/makaiquan/wenda/inc/       inc 每天的数据



```
zzzc_priv_qssl2eng", "10.173.92.202:2181,10.173.92.203:2181,10.173.92.218:2181
```





申请代码权限

https://cas.src.corp.qihoo.net/index.php/Auth/Permission/addIndex



flink流

[qdev - Revision 285538: /qsearch/branches/flink_stream (qihoo.net)](https://se.src.corp.qihoo.net/qdev/qsearch/branches/flink_stream/)





wiki (资料)

https://wiki.so.qihoo.net/pages/viewpage.action?pageId=137907748



hulk云平台(申请服务器)

https://hulk.qihoo.net/user/host/list#
密码: Nswdsm58@wh

dev103.se.corp.qihoo.net

dev106.se.corp.qihoo.net

offline01.qsst.zzzc.qihoo.net

hadoop dfs -ls /home/maintable/vertical/config/



运行flink， hulk中点击工作台，任务开发，实时任务

奇麟大数据

http://qilin.qihoo.net/#/scheduler/projects/query?type=create



查看已经申请的代码

[qdev - Revision 285538: /qsearch/branches/flink_stream (qihoo.net)](https://se.src.corp.qihoo.net/qdev/qsearch/branches/flink_stream/)

密码：nswdsm58





安装mvn

链接：https://pan.baidu.com/s/12SuxtOXiUhNgkb0DH1eyCw
提取码：gnaf

配置bin环境

使用mvn管理jar包

pom.xml配置

```
方式一：

mvn install:install-file -Dfile=E:\workspace\vertical_wenda\lib/flink-connector-kafka-0.9_2.11-1.11.1-f2-qihoo.jar -DgroupId=org.apache.flink -DartifactId=flink-connector-kafka-0.9_2.11 -Dversion=1.0.0 -Dpackaging=jar

<groupId>flink-connector-kafka</groupId><artifactId>flink-connector-kafka</artifactId><version>1.0.0</version>


import sahded.org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer09;

方式二：
把09kafka版本改为1.9.2
```



-Dfile : 源jar路径

-DgroupId : 相当于在C:\Users\jiangtao5\.m2\repository之后的路径

-DartifactId : 相当于-DgroupId之后的路径

这些值可以在flink中的pom.xml文件中看到





消费kafka数据

~~~
 ./kafka-console-consumer.sh --bootst
rap-server 10.173.194.164:39092  --topic zzzc_priv_qssl2eng_vertical_offl
ine_wenda_fresh  --group wenda-jt-test
~~~

## 写shell脚本

 cat  config.ini.new | awk '!/^#F/' | egrep -n "FIELD" | tail -n
  5| cut -d '|' -f1 | awk '($1>142)' | sort   > b.txt.sort

awk这里表示忽略以#F开头的行， egrep -n表示显示行号，cut -d ‘|’ -f1 表示以'|' 分割 后拿到第一段的数据

~~~shell
#/bin/bash

oldfile="$1"
newfile="$2"

 cat  $1 | awk '!/^#FIELD00=pdate/' | egrep -n "FIELD" | tail -n 1| cut -d '|' -f1 | sort  > a.txt.sort
str=`cat a.txt.sort | cut -d ':' -f1`
echo "老版本:"
echo "-------------------"
cat a.txt.sort
echo  "-------------------\n\n\n"

 cat  $2 | awk '!/^#FIELD00=pdate/' | egrep -n "FIELD" | tail -n 10| cut -d '|' -f1 | awk  -va=$str '($1>a)' | sort > b.txt.sort

echo "新版本"
echo "-------------------"
cat b.txt.sort
echo "-------------------"

~~~



readlink

~~~shell
readlink 是用来找符号链接所指向的位置的
例如
readlink -f /usr/bin/awk
结果为
/usr/bin/gawk #因为/usr/bin/awk是软链接

readlink -f /home/jt/log
结果依然是 /home/jt/log ，因为没有软连接



~~~

获取当前路径方法

~~~shell
#!/bin/bash
path=$(dirname $0)
path2=$(readlink -f $path)
#可以换成path2=$(readlink -f $(dirname $0))
echo path2
sh path.sh
/home/software


#直接这样
#CURDIR=$(dirname $(readlink -f $0))
~~~

字符串拼接

~~~shell
your_name="runoob"
# 使用双引号拼接
greeting="hello, "$your_name" !"
greeting_1="hello, ${your_name} !"
echo $greeting  $greeting_1
#hello, runoob ! hello, runoob !

# 使用单引号拼接
greeting_2='hello, '$your_name' !'
greeting_3='hello, ${your_name} !'
echo $greeting_2  $greeting_3
#hello, runoob ! hello, ${your_name} !
~~~

字符串输出

~~~shell
name="jt"
echo $name
echo ${name} 
#以上两种都可以， 但最好加上{}
比如
echo "I am $jtwhoareyou" # 错误方式
echo "I am ${jt}whoareyou"

~~~



多行注释

~~~shell
:<<EOF
...
...
EOF 

可以把EOF换成! 或者 ’
~~~

$类

~~~shell

$? 获得上一个进程的状态码(0表示成功， 1表示失败)
#如果上一句执行的是 echo "666" ,返回0则表示成功

$$ 获取当前进程ID

$! 获取上一个进程ID，#比如在脚本中穿插了其他脚本，或者使用了echo等命令， 这些都是其他进程做的

$# 获得参数列表的总个数
 
$@ 和 $* 表示输入的参数列表
#遍历输入参数
cnt = 1
#这里要不要双引号豆科野，如果是$*就不能要双引号
for i in "$#"  
do 
	echo "number of $cnt parameter is : $i"
	((cnt++))
done
~~~

在脚本中执行另一个脚本

~~~shell
sh -x ./lingyige.sh $1 $2
~~~

