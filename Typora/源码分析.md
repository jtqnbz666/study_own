### 网络框架分析

Nginx

~~~shell
master进程负责管理worker进程，包括启动、停止和监控worker进程。
worker进程负责处理实际的请求。

main函数在src/core/nginx.c中
1. bind和listen操作：在main函数的ngx_init_cycle中的ngx_open_listening_sockets可以看到。 # 如果设置了reuseport，并且work进程的数量设置为4，那么master会直接创建(bind+listen)4个listenfd，每个work进程执行了epoll_create后会放其中一个listenfd到自己的epoll中, 如果不设置reuseport那么master进程只会创建一个listenfd，这4个work进程共用同一个listenfd(也就是所有work进程共用一个监听队列，必须配合accept锁来避免惊群)
2. fork操作：main函数中有ngx_master_process_cycle函数，这里面的ngx_start_worker_processes里面的ngx_spawn_process执行fork子进程的操作
3. epoll_create操作：fork操作中ngx_spawn_process传递了一个函数指针ngx_worker_process_cycle，fork执行完后就会执行ngx_worker_process_cycle，进一步看ngx_worker_process_init，再进一步看cycle->modules[i]->init_process(cycle)，这个init_process绑定的是ngx_event_process_init函数，进一步可以看到module->actions.init，这个init函数绑定的是ngx_epoll_init，这里面执行了epoll_create, 然后把fork之前的listenfd放到各个work进程的epoll中，但由于多进程监听同一个listenfd，所以有惊群现象，需要配合nginx实现的accept锁来避免惊群问题。
~~~

328框架

~~~shell
IConnection对象封装socket，IConnectionManager管理IConnection IMessage表示消息内容， IRequest包含了IMessage和IConnection，每次IConnection对象read了数据之后封装为一个IRequest对象放到任务池(也就是协程池，包含了处理方法的映射表<commandID, handle类>，协程一拿到IRequest就能知道commandID)，通过IRequest对象的IConnection可以拿到连接，把处理结果发回去。

#网络框架
因为是用的go，没有epoll，针对每一条连接都单独开了一个协程去处理，while执行read，此协程只负责读，然后把数据封装为一个Msg对象进一步封装为Request(包含这个Msg对象和对应连接的信息)放到任务队列中。
# 感觉go层面的协程循环读跟c++中epoll触发事件再读的成本差不多
~~~

teamtalk网络框架

~~~shell
头和数据消息是分开的(头是自己定义的，数据消息是用的proto定义的，比如一条请求包含了消息头Header和请求消息LoginMsg)，会放到一个buffer上，先把头放进去，然后把数据消息LoginMsg压缩成proto追加到这个buffer的后边。 
# 为什么不把消息头也用proto，因为proto压缩后，消息头就不是之前的大小了(比如消息头是两个int，proto压缩后就不是八个字节了)，这样接收方就没办法取出消息头。


# 以下的方式可以把协议头读取出来，就不用逐个字节的去读了。
PduHeader header;
memcpy(&header, buf, sizeof(header));

// Convert network to host byte order if needed
header.length = ntohl(header.length);
header.commandID = ntohs(header.commandID);
header.seqNum = ntohs(header.seqNum);
printf("test:%d,%d,%d\n",header.length, header.commandID, header.seqNum);
~~~

