查看系统日志

``` shell
dmesg  # (diagnostic message)
# 查看是否系统kill掉的
dmesg | grep 'kill' # 配合-C看上下文的时间
```

shell配置文件加载时机

``` shell
# 登录shell
通过ssh，或者输入账号密码连接的方式为登录shell，会去加载第一个找到的文件，顺序为~/.bash_profile->~/.bash_login->~/.profile，
# 非登录shell
比如已经在终端中了，然后从bash进入了sh或zsh，这种为非登录shell，只会加载~/.bashrc。

# 注意点：
# ~/.bashrc登录的时候不会自动加载，一般会在登录shell的配置中如~/.profile中加载~/.bashrc，确保每次登录时非登录shell的配置也能正确加载，
```

ssh操作

``` 
shell命令中可以用ssh连接远程服务器执行某些操作。
```

端口转发

~~~shell
# 前置说明：
1.目标地址不是必须本地的地址，比如本地端口转发场景下，云服务器只是作为一个跳板。
2.比如远程端口转发，ssh -R 8000:127.0.0.1:8000 duoletest@101.42.21.34，输入sudo lsof -i:8000可以看到ssh建立了一条通道比如 127.0.0.1:52671->127.0.0.1:irdmi (ESTABLISHED)，那么远端服务器8000收到的流量在本机收到后，本机会以52671为源port进一步转发请求到目标服务，目标服务返回数据后ssh可以根据连接信息再回复给远端服务器，所以对两台远端服务器执行8000:127.0.0.1:8000是可行的，本机会在可选端口范围随机分配不同的源端口与远程服务器的8000对接
# 本地端口转发（本地访问云服务器的服务）
ssh -L [本地端口]:[目标地址]:[目标端口] [用户名]@[远程服务器]
ssh -L 6666:云服务redis的ip:6379 root@8.130.49.219
应用：本机直接连接堡垒机才能访问的redis，比如redis-cli -h 127.0.0.1 -p 6666 -a jC3cS0sA4mJ3就能直接连接到云服务的redis。

# 远程端口转发（云服务器访问本地的服务）
ssh -R [远程端口]:[目标地址]:[目标端口] [用户名]@[远程服务器]
ssh -R 7890:127.0.0.1:7890 root@8.130.49.219
应用：从服务器上走本地代理
在服务器上配置代理，会把请求转到本机
export http_proxy=http://127.0.0.1:7890
export https_proxy=http://127.0.0.1:7890

# 动态转发（以上两个都是指定端口，动态转发会转发所有流量到远端服务器的端口。
ssh -D [本地端口] [用户名]@[远程服务器]
ssh -D 1080 root@8.130.49.219
export ALL_PROXY=socks5://127.0.0.1:1080（ALL_PROXY是curl用的变量，比如ssh走的22端口流量不会检查这个变量，除非用Proxifier这样更高级的代理工具，或者说redis-cli也不会检查这个代理，一般用于http/https访问）
curl https://www.google.com  # 走8.130.49.219代理访问
应用：比如服务器是国外的，要通过这个国外服务器访问国外网站
# 动态转发流量说明：
浏览器 --> 本地SOCKS5代理 --> SSH隧道 --> 远端SSH服务器 --> 目标网站（`example.com`）
目标网站响应 --> 远端SSH服务器 --> SSH隧道 --> 本地SOCKS5代理 --> 浏览器


# ssh —D动态转发和公司的代理区别
公司代理Proxifier工具（比如远端的49.232.50.234:9013，转发了本地的22端口流量）：
是一个真正的代理服务器，在远端部署，本地连接这个代理，指定端口的流量都会经过代理服务器做转发。
ssh -D（动态端口转发）：
它是一个“本地中转站”，在本地开启一个SOCK5代理，所有支持SOCKS5的应用(比如浏览器，curl，wget、迅雷等)都可以配置连接这个本地代理，应用先将请求封装成socks5协议，然后发到本地socks5代理，本地socks5解析出来原本的内容，比如http完整信息，再把这份原始信息通过SSH隧道转发到远端。


# 场景：
1.公司的Proxifier强制把目标为22端口(也就是ssh服务）的所有流量都导到了代理服务器上，进一步连接线上的机器，如果在家不开VPN，直接启用Proxifier，ssh流量会被拦截到代理服务器，但由于代理服务器的22端口不对陌生机器开放（只能是公司ip或者挂VPN），导致无法ssh连接任何一个没有限制的云服务器。
2.比如有两个服务器A和B，有个web工具只能访问A，不能访问B，可以在A和本机建立端口映射，A的请求就会到本机上，然后本机再通过ssh命令连接B，在B上执行某些操作。

# 常见问题
# 为什么不是直接浏览器到ssh隧道而是需要socks5转发？
浏览器本身不支持SSH协议，无法直接建立SSH连接，未来如果浏览器原生支持SSH连接，理论上就可以直接通过SSH隧道发送请求，无需中间的SOCKS5代理层。
# 机器的ssh不让端口转发
/etc/ssh/sshd_config中修改
AllowTcpForwarding yes
GatewayPorts yes
sudo systemctl restart sshd # 重启ssh服务
~~~



xlsx转为csv

~~~shell
 pip install xlsx2csv
 xlsx2csv a.xlsx a.csv
 这样就看到的数据就是肉眼可见的了
~~~

清除环境变量

~~~shell
echo $http_proxy 
当前终端临时清除：unset http_proxy 
永远清除：修改对于的~/.bashrc或~/.zshrc，然后source更新
~~~

rm操作

~~~shell
rm -rf sj_game
├── 1.txt
├── sj_game
├── test.txt
└── tt
    ├── sj_game
    └── t1.txt
只会删除当前目录的sj_game，不会递归进入tt删除tt中的sj_game
~~~

正则操作

~~~shell
# 常用
[^:]* 不是贪婪匹配，从前开始找，找到第一个:时因为不符合匹配规则了就会停下来。
.*: 其中.*是贪婪匹配，直接去末尾，发现后边不是:，才会开始回溯往前找，所以如果如果有多个:，会匹配最后一个
.*表示模糊匹配，*匹配任意单个字符，*表示前边的字符可以重复0次或多次，
.+表示匹配至少一个任意字符不能是0个。
/test/ 匹配包含test的内容，比如awk '/test/'

# 基本字符匹配
.：匹配除换行符以外的任意字符。
\d：匹配任意数字，等价于[0-9]			（digit缩写
\D：匹配任意非数字字符
\w：匹配任意字母、数字或下划线，等价于[a-zA-Z0-9_] （word缩写
\W：匹配任意非字母、数字或下划线字符。	
\s：匹配任意空白字符（空格、制表符、换行符等）			（space缩写
\S：匹配任意非空白字符。 
# 数量修饰符
*：匹配前边的字符0次或多次（某些地方用作通配符，对于文件名的通配时不加''，比如grep、ll、tail
+：匹配前边的字符至少1次
?：匹配前边的字符0次或1次（http/https这种就可以使用）
{n}：匹配前边的字符恰好n次
{n,}：匹配前边的字符至少n次
{n,m}：匹配前边的字符至少n次，至多m次。
# 边界匹配
^：匹配字符串的开头，如果是在[]中，含义为非，表示不在这个集合中
$：匹配字符串的结尾
\b：匹配单词边界 （\bcat\b）会匹配"cat is here"中的cat
\B：匹配非单词边界 （\Bcat\B）会匹配"concatenate"中的cat
# 分组和选择
()：用于分组，或者作为捕获组，比如regexp_extrat(contentstr, 'userid=(\d+)', 1)表示提取第一个捕获组，也就是userid具体的值。
|：匹配左右任意一个表达式（或操作）
# 字符集和排除
[abc]：匹配方括号内的任意一个字符
[^abc]：匹配不在方括号内的任意一个字符
[a-z]：匹配任意小写字母
[A-Z]：匹配任意大写字母
# 特殊字符转义
\：用于转移特殊字符，比如\.匹配句点
~~~

mv操作

~~~
mv只是修改文件的名字，不会修改文件的创建日期，如果是cp的，那么新文件的创建日期是执行cp命令时。
~~~

core文件生成

~~~shell
1.ulimit -c unlimited # 默认是0

常见问题：
无法生成core文件
# 查看core文件的生成路径
sysctl kernel.core_pattern  # 可能是系统使用Apport上报了崩溃。
# 更新core文件生成路径
sudo sysctl -w kernel.core_pattern=core.%e.%p
~~~

前台后台运行(bg、fg操作)

~~~shell
case：命令行运行了一个程序，暂时暂停它用ctrl+z，如果要后台继续运行它就用bg + 序号(通过jobs查看)，如果要前台运行就用fg，如果要把它从当前终端分离出去(即jobs就看不到它了)用disown，如果这个程序还在输出内容到终端，尽管disown了，退出终端程序也会终止，如果没有输出了或者输出都被重定向到别的文件了，那么退出终端程序会继续运行。

# 可以输入screen创建一个虚拟终端，在虚拟终端里面执行disown，尽管还在不断输出，此时退出终端程序也能继续执行，可以理解为，退了终端之后，screen创建的虚拟终端没收到影响。
# bg和fg如果不加序号，默认操作的是当前作业，当前作业旁边有个+号。
# screen -ls查看有哪些虚拟终端，暂时离开screen先按ctrl+a再按d， screen -r 161185(虚拟终端号)可以重新连接上虚拟终端，ctrl+d关闭screen(如果有多个真实终端连接了该screen，则需要全部退出才会彻底退出这个screen)
~~~

重定向输入内容

~~~shell
tr -s ' ' '\t' < test.txt  # 内容就会从test.txt中拿，如果不要< test.txt，就从终端的输入拿值。
~~~

systemctl

~~~shell
systemd是管理器，负责管理系统服务，systemctl是命令行工具，提供了接口执行管理操作。
# 查看所有服务
systemctl list-units --type=service # --all 包含未允许的，配合grep过滤需要的
# 查看特定服务状态
systemctl status uwsgi.service
# 启动、停止、重启、开机自启、取消开机自动、重新加载(加载配置不中断服务)
start、stop、restart、enable、disable、reload
# 查看服务日志
sudo journalctl -u uwsgi.service
~~~

查看多少核

~~~
lscpu
~~~

查看目录的权限

~~~shell
ls -ld 目录名  	
-l 选项会显示详细信息，包括权限、所有者、组、大小和最后修改时间。
-d 选项确保只显示目录本身的信息，而不是其内容。
~~~

查看某个命令是否为别名

~~~shell
alias + 命令，如下
alias ll
~~~

监控系统进程的系统调用

~~~shell
ps -aux | grep '进程名' # 获取进程号
strace -p 进程号

#追踪某个进程的标准输出
trace -p 进程号 -e write=1 # 1是fd套接字表示标准输出

#追踪指定函数如exec函数
strace -f -e trace=execve ./wzqgame # -f表示跟踪由fork调用生成的子进程

直接用strace启动程序， 比如strace ./a.out， 要观察epoll程序是ET还是LT模式就很好用。
~~~

抓包tcpdump

~~~
tcpdump -i eth0 tcp port 9030
~~~

获取公网ip

~~~
curl ifconfig.me
~~~

更新文件时间戳

~~~shell
# uwsgi的配置如果时间戳更新，服务就会重启
touch --no-dereference test.ini
~~~

ls -l和find和grep模糊匹配场景

~~~shell
find sj_team*  # 默认一直递归
ls -l sj_team* 	 # 如果是目录只会去第一层
grep -w 'test' log/sj_teamsvrd*/sj_teamsvrd*log/sj_teamsvrd*.log # 不加-r不会处理目录
~~~

给history添加时间信息

~~~shell
# bash下
export HISTTIMEFORMAT="%d/%m/%y %T "
export PROMPT_COMMAND="history -a; $PROMPT_COMMAND"
# fish下(旧版本不行)
history --show-time='%F %T'

# fish还有别的问题
tail -f 如果是别名比如 taillog | grep 'test'， 后边的grep不生效，可以用taillog | awk '/test/'替代， /表示正则
~~~

ctrl+\和ctrl+c的区别

~~~
前者是信号3，后者是2，区别在于前者会生成核心转储文件，前提执行了ulimit -c unlimited
~~~

ll查看文件详细时间信息

~~~
ll --full-time
~~~

tail多文件监听

~~~shell
tail 1.txt 2.txt

tail log/sj_roomsvrd*/sj_roomsvrd*log/sj_roomsvrd*.log -f
~~~

mac下vim基础配置

~~~shell
set tabstop=4        " 设置一个制表符等于4个空格
set shiftwidth=4     " 设置自动缩进使用4个空格
set expandtab        " 将制表符转换为空格
set number           " 显示行号
set relativenumber   " 显示相对行号
set cursorline       " 高亮当前行
set background=dark  " 使用深色背景 # 白色主题用会太闪
set autoindent       " 自动缩进新行
set smartindent      " 智能缩进
set hlsearch         " 高亮搜索结果
set incsearch        " 增量搜索
set ignorecase       " 搜索时忽略大小写
set smartcase        " 当搜索模式包含大写字符时，自动切换为区分大小写
set wrap             " 自动换行
set linebreak        " 在单词边界换行
set showmatch        " 高亮匹配的括号
set wildmenu         " 命令行补全增强
set wildmode=list:longest,full " 补全模式
set clipboard=unnamedplus " 使用系统剪贴板
set mouse=a          " 启用鼠标支持（禁用了cv复制
set termguicolors    " 启用 24-bit RGB 颜色支持
~~~

xargs

~~~shell
# 用于将输入转换为命令行参数。
# docker也是同样的方式
ps -aux | grep "vscode" | awk '{print $2}' | xargs kill -9
~~~

htop操作

~~~shell
1.按H可以查看帮助规则。

查看进程占用资源情况
shift + p 以CPU消耗排序
shift + m 以内存消耗排序
shift + t 以进程运行时间排序
shift + h 以线程级别展示消耗
按c可以看到具体的command命令

#普通top命令
Mem行相关(按m可以以图形化形式展示比如80.5/3782688，第一个值是当前百分比，第二个是总大小)：used才是实际正在使用，free和buff/cache都是可用部分。
Swap航相关：avali Mem字段是系统保守估计的可用内存，比free+buff/cache小(包含free和部分buff/cache)，考虑了系统需要保留的缓存。swap空间是用于内存不够时，支持把部分内容从内存移动到磁盘上。
cpu相关(按1可以看到每个cpu的负载)：us、sy、id是几个比较重要的参数
us (user): 用户空间进程的CPU使用率。指用户进程（不包括 nice 值调整的进程）使用的 CPU 时间百分比。
sy (system): 内核空间进程的CPU使用率。指内核进程使用的 CPU 时间百分比。
ni (nice): 经过 nice 值调整的进程的CPU使用率。指那些通过 nice 值调整优先级的进程使用的 CPU 时间百分比。
id (idle): 空闲CPU百分比。表示 CPU 在空闲状态下的时间百分比。
wa (iowait): I/O等待的CPU百分比。表示 CPU 等待 I/O 操作完成的时间百分比。
hi (hardware interrupts): 硬中断的CPU百分比。表示处理硬件中断所用的 CPU 时间百分比。
si (software interrupts): 软中断的CPU百分比。表示处理软件中断所用的 CPU 时间百分比。
st (steal): 被虚拟机管理程序偷走的CPU百分比。表示在虚拟化环境中，其他虚拟机占用的 CPU 时间百分比。
~~~

通过进程号信息查看程序的可执行文件位置

~~~shell
1. 先通过端口号/进程名获取进程信息
lsof -i:port 或 ps -aux | grep 进程名 
2. 通过进程号查询完整路径
ls -l /proc/21144/exe 
3. 如果这个命令是通过别的命令启动的比如python就需要用pwdx命令
pwdx 21144
4.查看更详细信息，比如启动时间
stat /proc/21144
~~~

find操作：在指定目录下查找某个文件所在位置

~~~shell
find 查询的路径放在前边，grep查询的路径放在后边，其实用法基本一样，但find默认就是递归路径查找的，而grep需要加上-r参数, 如下，他们会匹配所有j开头的文件或者目录递归的查询
grep -r 'tt' j* # 如果不加-r， grep是不处理文件夹的
find j* 

find . -name "ranking*" # 注意*是模糊匹配，grep查询内容时的*是正则表达式，表示*前边的字符可以出现0次或多次，跟这里含义不同
find jt/ t* # 这并不是在jt目录下找t开头的，实际上是在查找 jt/ 和 t* 两个路径, 这样的结果是查询所有jt/下的和t开头的文件或者目录，并且是递归的查，-name才是搜索条件
find jt/ -type f -name 't*' # 查找jt下t开头的文件
find jt/ -type d -name 't*' # 查找jt下t开头的目录

# 查大于100M的文件
sudo find / -type f -size +100M -exec du -h {} + | sort -rh
#{}：这是一个占位符，表示当前找到的文件名。find 会将它替换为每个匹配的文件。
#+：与 -exec 结合使用时，+ 表示以批处理的方式执行命令，比如find的文件有/file1，/file2，/file3，相当于执行了du -h /file1 /file2 /file3, 如果把+换为 \; 就会对结果分别执行du -h 

# 指定递归深度(默认一直递归)
find . -maxdepth 1 # 只查当前目录

# 取反，比如不查询某个文件(配合grep的正则如-vE也行)
find . -not -name '*group*'
~~~

配置本地代理服务

~~~shell
# 本地有代理，但却访问不了外网的场景
export http_proxy=http://127.0.0.1:7890
export https_proxy=http://127.0.0.1:7890
~~~

base64、sha256sum、utf-8编码

~~~shell
echo -n "你" | 编码规则
# 常用编码规则：1.base64， 2.sha256sum， 3.od -t x1 (utf-8的)
echo "test" | base64 # 最后加上-d表示反解
# 打印unicode码点
echo -e '\u80dc\u52291\u5c40'(mac不行) 或 printf '\u80dc\u52291\u5c40'
~~~

path路径配置

~~~shell
# 比如要增加/usr/local/Homebrew/bin
export PATH="/usr/local/Homebrew/bin:$PATH"
~~~

动态库路径找不到

~~~shell
#比如动态库文件在/home/ConnServer下， export LD_LIBRARY_PATH=/home/ConnServer:$LD_LIBRARY_PATH无用的情况下
 echo "/home/ConnServer" >> /etc/ld.so.conf && ldconfig
~~~

权限管理

~~~shell
小知识：
1.一个文件只有一个所有者和一个所属组，但一个用户可以在多个组中
2.谁创建的文件，所有者就是谁，不管是在哪哪个用户的目录下创建的，比如A修改B的文件没有权限，但是因为A有sudo权限，保存的时候强制保存，则该文件的所有者会从B变为A，可以理解为覆盖了之前的，但如果是在外边用的sudo命令或者直接用root用户去修改,则不会修改文件所有者。

修改密码
passwd jiangtao # 第一次给root设置密码时用sudo passwd root
查看组的权限
cat /etc/sudoers
可以看到 %sudo   ALL=(ALL:ALL) ALL
%sudo: 指定 sudo 组。百分号表示这是一个用户组，而不是单个用户。
ALL=(ALL:ALL):
第一个 ALL：表示这条规则适用于所有主机。
第二个 ALL：表示可以以任何用户的身份运行命令。
:ALL：表示可以以任何组的身份运行命令。
ALL：表示允许执行所有命令
修改用户的sh为bash
sudo chsh -s /bin/bash jt #如果是sh，上键不是上一条命令而是一个符号，chsh(change shell) -s表示指定新shell路径
删除用户
userdel jiangtao
创建用户
useradd -m jiangtao # 必须要-m不然不会生成/home/jiangtao
usermod -aG sudo jiangtao #这样就可以用sudo权限了
查看所有用户
cat /etc/passwd
查看所有组
cat /etc/group
更改文件/文件夹的所有者和组
chown user:group 文件/文件夹
修改文件/文件夹权限
chmod 777 文件/文件夹
查看当前目录的权限
ls -ld .
查看用户所属组
groups username # 不加username默认看当前用户
创建组:
sudo groupadd docker
将jt从root组移除
sudo gpasswd -d jt root 
添加用户到指定组
sudo gpasswd -a 用户名 组名
sudo usermod -aG docker $USER	 # 表示当前用户加到docker组,a(append), G(groups), $USER是当前所在的用户
设置用户的组
sudo usermod -g newgroup username
组员身份立即生效
newgrp docker
# 默认创建的文件权限（umask为4位，第一位是特殊权限位，一般不用，看后3位
默认权限666 - umask的值
# 默认创建的目录权限
默认权限777 - umask的值
~~~

防火墙

~~~shell
阿里云的服务器，端口能访问必须满足两个条件，阿里云的安全组允许以及服务器内部的防火墙(iptables)允许，一般情况下打开阿里云的外部防火墙即可。

sudo iptables -L -n -v 查看所有防火墙规则
1.关闭3001端口访问:sudo iptables -A INPUT -p tcp --dport 3001 -j DROP(要注意如果docker的规则允许访问则还是能够访问，docker优先级更高， sudo iptables -L DOCKER -n --line-numbers查看docker规则， sudo iptables -D DOCKER 3表示删除列出来的第三条docker规则)
2.打开3001端口访问:sudo iptables -A INPUT -p tcp --dport 3001 -j ACCEPT
3.移除3001端口的阻止规则:sudo iptables -L INPUT -n --line-numbers查看有哪些规则，如果3001的在第二行，使用sudo iptables -D INPUT 2 表示删除第二条阻止规则
4.保存规则:sudo iptables-save > /etc/iptables/rules.v4
5.恢复规则:sudo iptables-restore < /etc/iptables/rules.v4
-A：追加一条规则到指定链的末尾 (Append)。
-D：删除指定链中的一条规则 (Delete)。可以通过规则号或规则描述来删除。
-I：插入一条规则到指定链的开头 (Insert)。
-L：列出当前的规则 (List)。
-n：以数字形式显示地址和端口，不进行 DNS 解析 (Numeric)。
-v：显示详细信息 (Verbose)。
--line-numbers：显示规则的行号。
-p：指定协议 (Protocol)。
--dport：指定目标端口 (Destination Port)。
-j：指定匹配该规则后的目标操作 (Jump)。常见值有 ACCEPT (接受包) 和 DROP (丢弃包)。
~~~

端口扫描

~~~shell
nmap -Pn -p 1-100 8.130.49.219 # 扫描1-100的端口
-Pn 选项告诉 nmap 跳过 ping 测试, 有些服务器禁止被ping

# 扫描udp端口12345
nmap -sU -p 12345 8.130.49.219
~~~

查看哪个程序占用内存较多

~~~
ps aux --sort=-%mem | head -n 10
也可直接htop然后按M
~~~

查看用户的所属组

~~~
groups username
~~~

更改文件的所有者和所属组

~~~
chown 用户:组名 example.txt

ssh中的authorized_keys权限必须是600
~~~

更新共享库的缓存

~~~
ldconfig
~~~

后台运行一个程序

~~~shell
./wzqauth > /dev/null 2>&1 &

2> 是错误输出重定向操作符，用于将标准错误输出（stderr）重定向到一个文件或设备。
&1 表示将标准错误输出重定向到与标准输出相同的位置。在这里，标准输出已经被重定向到 /dev/null，因此 2>&1 也将标准错误输出重定向到 /dev/null。
& 是后台执行操作符，用于将命令放入后台执行。
  
# 可以在命令前加上nohup明确表示忽略sighup挂起信号(sigint和sighup默认处理都是杀掉进程)，确保进程在终端关闭后照常运行(watch ls & 退出终端进程也被杀掉了，前边加上nohup退出进程也不影响), 并且如果没有指定输出位置则会把内容写到nohup.out中
~~~

打开当前目录

~~~
open .
~~~

命令行下载工具

~~~shell
wget: 简单易用，适合下载文件和整个目录。
wget https://example.com/file.tar.gz
一般用wget -O 下载后保存的名字 下载url # 注意是大写-O，小写会把日志写进去。

curl: 更多的配置选项，可以用来测试 API、发送各种 HTTP 请求等。
curl -O https://example.com/file.tar.gz

curl http://192.168.1.41:9001/userinfo?userid=390892&gameid=3 后边的路径用浏览器中填的内容就行。
~~~

连接服务

~~~shell
telnet ip port # 可以测试端口通不通，#连不上提示一直是Trying ...
nc ip port # nc -u ip port -u表示udp模式， 那服务器监听的端口也得是udp的，得用nc -lu port来监听udp端口， 注意防火墙要允许udp规则，连接成功的表现是没退出，没有额外的提示信息。

# 如果想用nc监听端口但没安装，可以用python来监听某端口
python3 -m http.server 8060 
~~~

查看端口详细信息(有几个监听队列等)

~~~shell
ss、netstat、lsof -i:

ss相当于升级版的netstat，参数的含义基本一致，ss 如果用了-l(-al也一样)就不会展示非listen状态的条目，而nestat除非只用-l,一旦用-al也会展示非listen状态的

-a：显示所有套接字。(如果不要，只展示establish状态的)
-t：仅显示 TCP 套接字。
-u：仅显示 UDP 套接字。
-n：以数字形式显示地址和端口。(不进行DNS解析，不会尝试把ip转为主机名，比如80可能被解析为HTTP，mac系统就得加上否则很慢。)
-l：仅显示监听的套接字。
-p：显示使用连接的进程 ID 和程序名（需要 root 权限）。

sudo netstat -tap | grep '1111' # mac下用 -nav参数， -v为verbose

sudo ss -tap | grep '1111' # 结果中的Recv-Q表示收到的连接请求(监听队列)的数量，Send-Q的值是调用listen()函数时设置的值

lsof -i:1111 # 可以查到某个端口所有的连接状态，如果是多个进程复用的同一个监听队列(也就是说本质上只bind了一次，其他进程全是bind之后fork出来的)，虽然可以看到有多条信息，但它们的DEVICE字段值是一样的，说明它们用的是同一个监听队列， 但如果是在每个进程中单独bind的就可以看到它们的DEVICE字段值不是一样的，则是多个监听队列，可以用ss -lp看出来是多个监听队列
~~~

当磁盘满了时排查是哪个文件夹过大

~~~shell
# 查看当前磁盘使用情况
sudo df -h 
# 查看当前目录磁盘使用情况
sudo du -h

sudo du -sh /* | sort -h
# -s(summarize) 参数表示只显示最外层总计大小，-h(human-readable) 参数表示以人类可读的格式（如 K, M, G）显示大小，如果想要查看某个特定目录（例如 /var）下的内容，只需将 /* 替换为 /var/* 或其他路径

说明：
1.文件系统以块为单位分配空间，一个块通常是4k，比如一个文件内容1k都不到，但du -sh会看到是4k，因为这个命令就是看文件占用的磁盘空间的，而ll是看实际大小的
2.有时候df -h看到的文件比du -h看到的更多，是因为比如文件在磁盘上已经删除了，但由于某些采集日志的进程还在使用该文件，导致内核未释放，通过sudo lsof | grep deleted可以看到是哪些文件被删了但未释放。
~~~

使用ip address替代ifconfig，ip address更加详细。

含有很多网络工具的镜像

~~~shell
docker run -d --name egg1 --hostname egg1 praqma/network-multitoo
# 自带nginx服务，在 /usr/share/nginx/html/index.html修改默认显示内容， 可以使用curl localhost即可看到默认显示内容
~~~

grep配合xargs过滤后执行操作

~~~shell
# kill掉查询到的服务
ps -aux | grep _server | awk '{print $2}' | xargs kill -9 
~~~

查看远程仓库的url地址

~~~
git remote -v
~~~

新的服务器初始化root用户的密码

~~~shell
在root账户下执行passwd即可设置 # 进不去root账户使用 sudo su root，默认密码应该是开机密码
~~~

ssh设置服务器断开客户端连接时间

~~~shell
修改 /etc/ssh/sshd_config
ClientAliveInterval 60
ClientAliveCountMax 20
# 以上参数表示每60s检测一下，最多20次没有反应则断开连接，所以就是20分钟不操作就会断开连接。
systemctl restart sshd
~~~

配置多个密钥对

~~~shell
1. ssh-keygen -t rsa # 如果之前已经存在密钥对了，这次就会提示你重新命名，比如id_rsa_328,就会生成对应的id_rsa_328和id_rsa_328.pub
2. 在~/.ssh/config文件配置以下内容
Host gitlab.hoxigames.com
    HostName gitlab.hoxigames.com
    User git
    IdentityFile /home/deploy/.ssh/id_rsa_328
    StrictHostKeyChecking no
    IdentitiesOnly yes
# 那么拉取328的代码的时候就会走id_rsa_328这个私钥，前提是把id_rsa_328对应的公钥放到gitlab中了。
# 还有一个要注意的是公钥默认会去读id_rsa.pub而不是id_rsa_328.pub
~~~

拿到主机的id_rsa和id_rsa.pub文件就可以获取代码仓库的代码

~~~shell
1. 比如线上机器的公钥配置在了自己搭建的gitlab中，把本机的这两个文件换成线上机器的文件内容，那么本机就可以拉取gitlab的代码了
2. ssh登录服务器也是同样的原理
~~~

运行服务器使用密码认证登录而不仅仅是publickey认证

~~~shell
sudo vim /etc/ssh/sshd_config
PasswordAuthentication yes # 设置为yes并保存退出
sudo systemctl restart sshd
~~~

vim操作

~~~shell
复制10行:10yy， 用p黏贴，从当前光标的下一行开始黏贴
删除单行：dd
删除全文：ggDG
定位到某一行：:100回车
显示行号：set nu

删除区域内容：control+v选择后按d, 直接用v选能删除换行符
操作前进：control+r
操作后退：u
向右以单词移动：w
向左以单词移动：b

搜索操作：
1.当前行(不包含)开始/向下搜，？向上搜，提示文字看后半段就行（如 at TOP）
2.支持正则搜索同时匹配多个字符串，加上.*（如/foo.*boo )
3.满足一部分即可，加上\|（如/foo\|boo )
4.忽略大小写，在最后加上\c，（如/foo\|boo\c )  # c是case的意思
5.全字匹配，用\<\>围起来，（如/\<jiangtao\> )

# 实操
2核4G的机器上vim一个40m的文件，一个cpu就被打满了，很卡，而8核16G的机器上vim一个200多M的文件，cpu脚本没有变化，没有一个被打满，并且浏览不卡顿，所以vim还是有影响的，但机器性能好则可以忽略。
~~~

netstat常用参数

~~~shell
使用命令前先安装net-tools
-a：显示所有活动网络连接及监听端口。
-p：显示每个连接或监听端口相关的进程信息。
-c：持续输出网络状况，直到用户中断它为止。
-n：显示所有连接和监听端口的IP地址和端口号。
-l：仅列出正在监听的服务状态（listen状态）

# 常用场景，知道端口查进程或者知道进程查端口
netstat -ap | grep mysql
netstat -ap | grep 3306

# 最常用 anlp 一起加上
netstat -anlp | grep 3306
~~~

host文件位置

~~~shell
# linux下
/etc/hosts
# windows的host文件
C:\Windows\System32\drivers\etc
~~~

创建多级目录

~~~shell
mkdir -p /A/B  /A/C
~~~

服务器时间 

~~~ shell
# 查看
date
# utc时间查看
date -u
# 设置时间
sudo date -s "2024-04-02"
# 更精准
sudo date -s "2024-04-02 12:00:00"
sudo timedatectl set-time "2024-10-01 00:00:00" # 也行
# 恢复时间
sudo ntpdate time.nist.gov # 更精准
sudo ntpdate pool.ntp.org # 时间服务器池，选择最近的
~~~

磁盘剩余容量

~~~
df -h
~~~

文件压缩与解压

~~~shell
tar.gz
压缩：tar -zcvf archive.tar.gz file1 file2
解压：tar -zxvf archive.tar.gz

zip（无损压缩，支持压缩多文件）
压缩：zip archive.zip file1 file2
解压：unzip archive.zip

gzip(通常配合tar一起，压缩效率更高), .gz文件可以直接用zcat看数据
压缩：gzip file # 只能针对单文件进行压缩
解压：gunzip file.gz
~~~

tree操作

``` shell
tree -L 1 # 深度为1
```

watch操作

~~~shell
watch -n 0.5 "ps -aux | grep 'sj_team'"
watch -n 0.5 tree  监视tree命令，每0.5秒刷新一下
~~~

统计行数

~~~
wc -l 统计结果行数，实际上计算的是\n的数量，如果把最后一行的换行符删了，就会少一条结果
wc userlist.txt
~~~

奇怪的配置保存方法

~~~
control+o保存再control+x退出
~~~

密钥配置

~~~shell
密钥配置：ssh-keygen -t rsa 一路回车， 在~/.ssh下id_rsa.pub是公钥文件，id_rsa是私钥文件， 把自己的公钥拷贝到远端的~/.ssh 中的authorized_keys文件(没有就创建)

# hoxi这边是先在gitlab开个自己的账号， 然后把本地的公钥放到gitlab上
~~~

杀死某个进程比如nginx

~~~shell
pidof nginx # 可以看到nginx有哪些进程
kill -USR2 $(pidof nginx)
~~~

常用设置tab间距方法

~~~shell
编辑 ~/.vimrc   # rc是run commands 的意思
set tabstop=4			# tab键
set shiftwidth=4 	# 缩进
set expandtab 		# 将制表符替换为等价的空格

# 小tips:也可以直接编辑器输入以下内容
:set tabstop = M 那么一个缩进TAB的作用就是M个空格
~~~

配合管道批量操作mysql/redis等

~~~shell
echo 'keys *'xargs | docker exec -i data-redis redis-cli #注意不是-it

以下是一个删除指定redis数据的方法
echo 'SCAN 0 MATCH AIMirror_KnockoutPool* COUNT 10000' | redis-online-mirror > AIMirror_KnockoutPool.txt

cat AIMirror_KnockoutPool.txt | awk '{print "UNLINK " $0}' | redis-online-mirror
~~~

nslookup：

~~~
用于查询域名系统（DNS）的记录。它允许用户输入一个域名，并返回与该域名相关的IP地址、域名服务器等信息
~~~

给linux的操作取别名

~~~shell
~/.bashrc文件修改
alias mysql-online='mysql -uroot -p -hrm-bp1c4m1w22oq77q82.rwlb.rds.aliyuncs.com'
那么直接使用 mysql-online就能连接上数据库了

如果使用了fish的话， 需要把这些alias放到~/.config/fish/config.fish文件中
比如alias 1="ssh root@47.110.160.107"， 不要用''
~~~

大文件读取对比

~~~ shell
vim、nano是编辑器，会加载整个文件，是危险操作，避免操作大文件。
cat、tac、head、tail、grep、sed、awk都不会加载整个文件，cpu和内存变化不会太大，主要的影响是对于磁盘的，他们都是先读磁盘块->处理行，如果一行(以\n分割)的内容太长，内存占用通常只与单行最大长度有关，而与文件总大小无关，他们的读取方式基本一致，主要区别在于拿到数据后的处理方式不一样。

# cat、tac、head、tail、grep、sed、awk处理流程
cat:几乎不处理，快速倾倒数据。
tac:反智能处理，比cat缓和，因为步骤比cat复杂。
grep:扫描换行符做正则匹配判断。
head:计数处理，达到-n的数量就退出。
tail:与head类似，需要先计算seek到哪里才能恰好读到n行。
sed:只要不用保持空间保持大量数据就安全。
awk:编程式语言，处理比较灵活，sed和awk都一样，只要不要大量记录到内存就安全。

# cat和tac对比
cat比较危险，它的读取和输出速度几乎只受限于磁盘和CPU的速度，是全力冲刺，而tac会跳转(seek)->读取(read)->处理(procses)，整体吞吐量远小于cat，节奏较缓，给终端喘息的机会。

# 场景分析x
# cat largefile | head -n 1
cat 读一点 -> head 拿到 -> head 读够了（1行）就退出 -> cat 尝试再写 -> 发现管道已断，cat 也退出。
# 避免无效cat
cat file | grep 'test' ,这样会启动两个进程，cat其实是多余的。
# 指定时间段
格式为: 2024-01-02T05:47:00+00:00， 看4.20-4.30
awk -F"[T:]" '$2==4 && $3>=20 && $3<=30' logfile.log
格式为：2025-07-18 18:08:32,209
awk -F, '$1 >= "2025-07-18 18:08:00" && $1 < "2025-07-18 18:10:00"' logfile.log
grep -E '2025-07-18 18:08:[3]+' logfile.log, 
~~~

tar 

~~~
-z 表示使用 gzip 压缩格式。
-c 表示创建新的 tar 存档。
-v 表示在终端显示压缩的过程，以便查看详细信息。
-x 用于解压缩
-f 用于指定要操作的文件。
解压:tar -zxvf 
压缩文件:tar -zcvf myfiles.tar.gz myfile1.txt myfile2.png
以上命令将创建名为 myfiles.tar.gz 的压缩文件，其中包含 myfile1.txt 和 myfile2.png 两个文件。
~~~

comm操作

~~~shell
# 注：如果1.txt有两行456,2.txt只有一行456，那么第一列还会有一行456，除非自己手动uniq去下重
# common的缩写，求文件的差异，输出分为三列，使用格式：comm [option] file1 file2
# 要求是排序的文件才能对比，用sort排序时不用加-n，用默认的字典排序即可。
常用option如下
-1：不显示第一列（只在 file1 中的行）。
-2：不显示第二列（只在 file2 中的行）。
-3：不显示第三列（同时在 file1 和 file2 中的行）。

#case:
# 输出前两列，第一列输出只在file1中的，第二列输出只在file2中的
comm -3 file1 file2 
# 把只在file1中的放到file2中
comm -23 file file2 >> file2
~~~

sort操作

~~~shell
#case:
1.原地对文件内容排序，sort -n file.txt -o file.txt # 去重就用-u



字段号从1开始, ::1:2 这里面的1就是第三个字段
sort -k(key) 4 表示指定以第四个字段来排序，默认第一个字段

sort -t(delimiter) ":" 表示以':'分割字段，默认以连续的tab或连续空格分割

sort -n(numeric) 以数字大小排序，而不是字典序，默认是字典序

sort -r(reverse) 表示逆序排序， 

sort -u(uniq) 去冲

比如数据是
qwqee
niha2
1qwe

sort -n 结果如下
niha2
qwqee
1qwe

而直接sort结果如下
1qwe
niha2
qwqee
~~~

uniq操作

~~~
因为uniq会考虑连续与不连续的问题，所以一般与sort一起使用
一般是 sort filename | uniq -c

-c	统计 连续 重复的行出现的次数，并且删除重复的行
123
12341
123
123
uniq -c 结果为
1 123
1 12341
2 123


-u 显示出现一次(即不连续重复)的行，不会全局考虑，也就是不管以前出现过没有
比如原数据为
123
123
123142
123
123
123142
sort -u 结果为
123142
123142

但如果原数据为
123
123142
123
123
sort -u 结果为
123
123142


-d 仅显式连续出现的行
123
123
1234123
123
123
sort -d 结果为
123
123


~~~

tr操作

~~~shell
tr 一般后边跟两个字符集 ，用单引号双引号都可以
默认是-t参数，也就是将字符集1转化为字符集2
# -s 为 squeeze挤压
1.小写转大写
echo abc | tr 'a-z' 'A-Z' //ABC
echo "thirrrr is firhhhht" | tr  'rh' 'sa' //taissss is fisaaaat
可以看出转换关系是一一对应的
2.将a 转化为 A
echo abc | tr 'a' 'A'  //Abc

-s 将重复出现的字符串压缩为一个字符
1.去除空白行
cat file | tr -s '\n'
也可以用grep -v  "^$"
2.将重复出现的字符压缩并且替换为指定字符
echo "thirrrr is firhhhht" | tr -s "rh" "s"  //this is fist
//这个需要注意它把rhhhh当成一个整体转化为一个s

echo "thirrrr is firhhhht" | tr -s "rh" // thir is firht
~~~

cut操作

~~~
cut -b 以字节为单位进行分割，仅显示行中指定直接范围的内容
cut -d 自定义分隔符，默认以tab分隔,不是空格（如果要空格分隔就用awk的-F），必须配合-f或其他,不能单独存在
cut -fn 表示分割后的第n部分内容。 下标从1开始
cut --output-delimiter=' ' 更改输出内容的分隔符
cut --complement

要注意-d并不是把原字符串拆分了， 只是提供给用户一个访问某个字段的方法，原字符串本身没有变化(指的分隔符: 并没有被删除)
比如 "a:b:c:d:e:g:h:i"
cut -d ":" -f1  //a 
cut -d ":" -f 1-4,6,7 //a:b:c:d:g:h 逗号中间不要留空格

把这个分隔符: 变换为自定义的输出，比如 空格
cut -d ":" -f 1-4,6,7 cut --output-delimiter=' '

排除指定字段
cut -d ":" --complement -f 1-4,6,7  //e:i

截取字符串
i = 13145
echo $i | cut -b 3-6 //145 下标从1开始
expr substr $i 3 3	//145 下标从1开始
~~~

split操作

~~~shell
文件拆分
-l 以行数拆分
-b 以大小拆分

split -l 10 原始文件 拆分后文件的前缀
拆分后原文件会保留。

# 以大小分割
nohup split -b 1048576k -d data.20240101.log 20240101/ &
#nohup表示忽略sigup信号，使用&符号可以将命令放置在后台运行,20240101/表示切割后的文件放到这个目录下
ps -aux | grep "split"找到他可以kill掉

# 以时间段
比如日志格式为，2024-01-04T10:44:13+08:00
mkdir 20240103
awk -F'T' '{print >> ("20240103/data."$1"-"substr($2,0,2)".log")}' data.20240101.log
~~~



### 小技巧

~~~shell
echo test | base64 编码
echo dGVzdAo= | base64 -D 解码
echo -n "xx" | md5sum   # 不可逆，验证数据完整性, -n表示去掉xx后边的换行符，默认会加上换行符，并且-n需要在字符串之前，不能在后边。
echo -n "xx" | sha1sum

如果遇到无法修改/bin下的文件，就修改环境变量，比如mac中就是在～/.zshrc文件中(如果是在bash那么就是~/.bashrc)修改一下环境变量，这样就不用强制一定去/bin下找了，比如
export PATH=$HOME/bin:/usr/local/bin:$PATH:/opt/homebrew/bin

vim中set list 显示不可见字符， set nu 显示行号

docker logs的时候加个-f 持续看输出内容

在etc/profile 修改配置，就是永久有效的


进程三件套：
ps -aux | grep _ser  通过部分名字查看进程信息，-aux 比 -ef 信息详细
ps -p 11361 -o comm= 通过进程号查看进程名
lsof -i:3306		通过端口号查看进程信息。# 有时候看不到，前边加上sudo


全选vim， v + gg + G
复制全部  ggyG
删除全部  ggdG

移动行首 home , 行尾 end

查看ubuntu版本  cat /etc/issue
制表符\t 会自动帮你控制距离

echo -e "test\ntest" // 表示将\n转义字符输出，默认不输出
echo -ne "test" //-n表示输出之后不换行，默认是换行的

 netstat  通过端口号查一些信息，或者服务名查端口号
grep -
lsof -i:4369  通过端口查看这个进程的相关信息 
nc -l 4369 开启一个端口号

ps -aux | wc -l  统计进程数量

默认的>是只管stdout
2> 表示只管stderr , 2>> 表示只管stderr同时追加写。
命令 > 文件 2>&1  //将stdout和stderr覆盖的方式定向到文件， 把第一个>改为>>则表示追加(不要改第二个>)， 可重定向到/dev/null丢弃打印内容

~~~

tail操作

~~~shell
tail -f filename # 持续监控文件
tail -n +2 filename # 从正数第二行开始
tail -n 2 filename  # 从倒数第二行开始
~~~

**linux三剑客** awk(利用正则做一些操作，比如得到数据的第几列)  grep(查询文本信息) sed(替换文本信息)

~~~shell
（文本统计）
b.txt 内容
http://www.baidu.com/index.html
https://www.atguigu.com/index.html
http://www.sina.com.cn/1024.html
https://www.atguigu.com/2048.html
http://www.sina.com.cn/4096.html
https://www.atguigu.com/8192.html



cat b.txt | cut -d '/' -f3 | sort | uniq -c | sort -nr
# cut -d "/" -f3  用"/"作为分隔符，截取第个3字段
# sort 第一次排序
# uniq -c 显示该行重复次数
# sort -nr 按照数值(n)从大到小排(r逆序)

最终结果
3 www.atguigu.com
2 www.sina.com.cn
1 www. baidu.com


（ ip 连接数统计并排序）
netstat -an | grep ESTABLISHED | awk '{print $5}' | cut -d ":" -f1 | sort -n | uniq -c | sort -nr
~~~



awk操作

~~~shell
awk语法: awk 'pattern {action}' filename
pattern: 匹配条件（可以省略，表示匹配所有行）。
action: 对匹配行执行的操作（可以省略，表示打印匹配的行）。
比如-F, -v不需要放在''中是因为它们是命令行参数而不是awk脚本的一部分
test.txt内容, 124为空行
1  
2
3  ilovehh
4
5  you support

打印test.txt文件内容的每一行的最后一个字符串，如果只有一个字符串，那么就直接打印出来
awk '{print $NF}' test.txt # ${NF-1}则是倒数第二个， $NF代表最后一个字段，会打印每行的最后一个字段。
#ilovehh 
#support

输出每行数据的列数 
awk '{print NF}' # 0 0 1 0 3 每行一个数字，NF是一个内置变量

输出每行数据的行号
awk '{print NR}' # 1 2 3 4 5 每行一个数字，如果写$NR，那么就会打印第一行的第一个字段，第二行的第二个字段，依此类推

打印每行全部内容(多个连续空格算为一个，但指定了逗号就会严格区分一个逗号)
awk '{print $0}' #'{print $1}'每行第一列内容

打印特定行
awk '{print $1, $3}'

条件语句
awk '$1 > 100 {print $0}', 如果是等于必须用==，一个=为赋值。

定义变量
awk -v x="jt" -v y=666 '{print x, y}'

累加某一列的值
awk '{sum += $1} END {print sum}' filename # END表示只在最后输出总值。

cat test.txt | awk 'NF%2==0' #过滤掉奇数行
# 以，分隔打印第一个字段。(单个，)
awk -F ',' '{print $1}' test.txt
# 多个分隔符作为分隔符
awk -F '[, ]+' '{print $1}' test.txt # +号表示支持多个连续的逗号或者空格，如果没有+号就以单个符号分隔，比如',, '会被认为是一个分割标志，不要[]就表示严格以逗号紧接空格为分隔符
# 多个字符串作为分隔符
awk -F 'ID = |)' '{print $2}'

# 像grep一样匹配某些行
taillog | awk '/address/' # 匹配带有address的行，taillog是一个tail -f文件的别名
~~~

sed操作(Stream EDitor (流编辑器) )

~~~shell
# 提取特定符号后的数字
sed -E 's/[^:]*:([0-9]+).*/\1/' test.cpp，比如内容为2025-07-18 18:08:32,209，会得到08，如果改为sed -E 's/.*:([0-9]+).*/\1/'，则得到32，因为第一个.*是贪婪匹配，会找到最后一个符合条件的，\1表示第一个捕获组，-E是正则，比如test:后边是账号，sed -E 's/test:*([0-9]+).*/\1/' test.cpp即可提取，后边这个.*不可缺
#替换:
sed -i 's/,/ /g' input.txt # -i(in-place)表示直接原文替换，s(substitute)表示替换,g表示全局, ///分别放被替换和替换后的内容(mac中：sed -i '' 's/,/ /g' input.txt)
# 删除空白行
sed -i '/^$/d' input.txt # ^$是空白行，d是删除的意思
# 删除引号
sed -i 's/\"//g' test.txt
# 查看文件的指定中间行
sed -n '5,10p' filename #这样就可以只查看文件的第5行到第10行。
sed -n '2,$p' filename #第二行到最后一行，或者tail -n +2,
~~~

grep操作

~~~shell
说明：
1.500M的文件grep起来也是比较轻松的
grep即使文件很大一般也没问题，不像while(1)，是因为grep有很多io操作，这期间cpu就空出来了，而while(1)空循环没有这些io操作会直接打满一个cpu，如果在while(1)中加上io操作如打印，就跟grep基本等价。
2.如果*在被查询的内容中为正则用法，如果是文件名中表示通配符(比如grep 't*t' * 前者是正则语法，后者为通配符。)

常用：
-C 2 'test' 内容及其前后两行（不会有重复内容，比如-C 2，第一行和第三行都是匹配的内容，第二行不会出现两份）
-H 'test' 内容及其所在文件名
-h 只展示内容，不展示文件名


-i：忽略大小写，使搜索不区分大小写。
-v：反向匹配模式，只显示不匹配的行。
-n 或 --line-number：输出匹配行的行号。
-c 或 --count：只输出匹配的行数而不显示匹配的内容。
-r 或 --recursive：递归搜索目录及其子目录下的文件。
-l 只列出包含搜索内容的文件名
-E (Extended Regular Expressions, ERE)  使用更强大的基于正则表达式的模式匹配引擎，支持更多的正则表达式
-P 也是正则，比-E更强，支持比如(\d+)这样的解析。
-w 表示查找包含整个单词的行
一般可配合 wc -l 进行匹配行数统计
搜索不包含以下任一字段的内容 grep -vE 'SelectHero|InBattle|IsMatching|WaitingForPreparation|InTeam|Watching|InAdventureBattle'   不要-v就是包含任一字段的意思
如果不用-E参数则必须在|前边加上转义\，如grep 'jt\|wh'



举例:
1.使用 -r 递归搜索
grep -r "jiangtao" . (grep当前目录所有文件，可以不加. ，默认当前目录递归)
结果：./tt2:jiangtao， 左边是文件名，右边是内容
2.-r 和 -E配合
grep -rE "jiangtao|test" . #表示递归搜索当前文件夹下所有文件， 筛选出所有包含jiangtao或test的行 // ""jiangtao"|"test""也行
如果查询的内容中含有|等特殊符号，需要使用转义字符, 例如查询"jiangtao|"
grep -rE "jiangtao\||test" .
3.-r 和 -l配合
grep -rl "jiangtao" .
结果:	./tt2
4.查询多文件
#使用通配符
grep "390892" log/sj_fkldgamesvrd*/sj_fkldgamesvrd*log/sj_fkldgamesvrd*.log
grep "390892" *.txt
# 多个文件
grep "390892" file1.txt file2.txt
5.查询包含部分字段
ps -aux | grep 'sj_g.*1' 表示sj_g开头，中间可以有任意多的字符，并且包含1字符，比如 sj_game1svrd就会被匹配上
如果写成 grep 'sj_g*'则会匹配sj_后边有0个或者多个g的内容，比如sj_，sj_g，sj_gg都会匹配上，因为*表示前边的一个字符可以出现0次或n次。
6.配合()和+
grep -E '(test1|tes1t1)+:' test.cpp，只要是test1或tes1t1开头的就能匹配，+换成*则这两个字段可以没有，因为*表示0个或多个。
~~~

scp命令

```shell
# 远程文件夹到本地
scp -r root@47.110.160.107:/home/deploy/jiangtao/redis  .
```

gdb调试

```shell
# 持续有输出的进程可以输出重定向
run > output
# 连接正在运行的进程。
gdb attach / -p + pid ， # 用detach来分离进程不会影响原进
# 美化输出
set print pretty on
# 修改变量
set var i = 1
# 用core文件调试
gdb ./sj_gamesvrd6 core.6872 
# 看bt打印出来的某一帧信息，比如看#3的详情。
f3 
# 当前位置
where 查看当前位置
# 回退
rs 退后一步(reverse-step)
show directories 查看源文件搜索路径
directory path  设置搜索路径
# 增加调试信息
如果是用cmake编译的，需要加上-g参数才可以调试,比如(target_compile_options(MyProject PRIVATE -g))
# 查看函数代码
l funcname
# 实时看代码
先按control + x ，放开后按a
# 移除断点
b ：break 断点
方式1： b + 行号，方式2 b + 函数名
方式3: 若需要断点的地方不在主源文件，而在某个其他源文件中， 比如需要在其他源文件的 void test(int a);
这一行断点， 则直接 b void test(int )即可(我发现含有IM::BaseDefine等作用域参数的函数不能使用这个方法)
方式4：同样是在其他文件中， b test.cpp:22   即表示在test.cpp中22行断点

d ： disable  清除断点 
d + 具体断点数字

c : continue
继续执行， 直到遇到阻塞(比如cin >>) 或者下一个断点

r : run
一般刚调试时，需要先设置好断点， 然后 r 表示运行程序

n ：一行一行执行
s : 一行一行执行，但若是一个函数，会进入这个函数

p : print 打印	
p + 具体的变量		//可以看到打印出类的private 变量

bt ： 查看当前栈的情况，每一个调用都是一帧，可以通过f 帧序号来详细看具体的某一帧

多线程：
info break ：查看断点情况
info thread : 查看有哪些线程
thread + 数字 : 表示进入某个线程
thread apply 数字 命令 : 如 thread apply 2 n  
表示线程2中，执行下一行， 如果你直接n ，则当前你在哪个线程就执行哪个线程的下一行。

set scheduler-locking off : 运行全部线程
set scheduler-locking on : 只运行当前线程
因为很多线程同时运行可能不方便查看信息

```
